{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 33: Principal Component Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 05/25/21\n",
    "- onl01-dtsc-ft-022221"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gain an intuitive understanding of PCA and eigenvalue decomposition.\n",
    "- Understand how Principal Component Analysis reduces dimensionality.\n",
    "\n",
    "\n",
    "- **ACTIVITY: PCA with NHANES**\n",
    "    - Compress all 1800+ features of the [National Health and Nutrition Examination Survey (NHANES)](https://www.kaggle.com/cdc/national-health-and-nutrition-examination-survey) down to <10 features.\n",
    "    - Use PC features to find groups of people in 3D space.\n",
    "    - Tomorrow: use clustering algorithms to statistically identify groups of people. \n",
    "- **ACTIVITY: Follow-Up Feature Selection for Predicting Parkinson's Disease**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Videos:\n",
    "    - [PCA YouTube Playlist - With statquest and ThreeBlueOneBrown Videos](https://www.youtube.com/playlist?list=PLFknVelSJiSzgzNCV-Wvvk5R8PY2UNype) \n",
    "    \n",
    "- Readings:\n",
    "    - [In-Depth Article About the Curse of Dimensionality](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)\n",
    "    - [Article: Gentle Introduction to Eigenvalues and Eigenvectors for Machine Learning]( https://machinelearningmastery.com/introduction-to-eigendecomposition-eigenvalues-and-eigenvectors/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T20:16:25.818264Z",
     "start_time": "2020-04-28T20:16:25.814202Z"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type of Learning\n",
    "- Unsupervised\n",
    "\n",
    "#### Assumptions\n",
    "- Correlation among features\n",
    "\n",
    "#### Advantages\n",
    "- Captures most of the variance in a smaller number of features\n",
    "\n",
    "#### Disadvantages\n",
    "- Number of principal components that explain most of the variance are determined by the USER\n",
    "\n",
    "#### Requirements \n",
    "\n",
    "- Features must be scaled (StandardScaler)\n",
    "- Sensitive to missing data.\n",
    "- Sensitive to outliers.\n",
    "\n",
    "#### Example Use\n",
    "- Reducing feature space/dimensionality\n",
    "- Preprocessing\"\n",
    "- Creating a few, informative variables from tons of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the \"curse of dimensionality\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading: [In-Depth Article About the Curse of Dimensionality](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)\n",
    "\n",
    "<!-- \n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-curse-of-dimensionality-online-ds-pt-100719/master/images/sparsity.png\">\n",
    "\n",
    " -->\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://www.visiondummy.com/wp-content/uploads/2014/04/1Dproblem.png\">\n",
    "\n",
    "<img src=\"https://www.visiondummy.com/wp-content/uploads/2014/04/overfitting.png\">\n",
    "\n",
    "<img src=\"https://www.visiondummy.com/wp-content/uploads/2014/04/3Dproblem.png\">\n",
    "\n",
    "<img src=\"https://www.visiondummy.com/wp-content/uploads/2014/04/3Dproblem_separated.png\">\n",
    "\n",
    "> ...\n",
    "\n",
    "<img src=\"https://www.visiondummy.com/wp-content/uploads/2014/04/sparseness.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does PCA solve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-pca-in-scikitlearn-online-ds-sp-000/master/images/inhouse_pca.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for Performing PCA\n",
    "\n",
    "The theory behind PCA rests upon many foundational concepts of linear algebra. After all, PCA is re-encoding a dataset into an alternative basis (the axes). Here's the exact steps:\n",
    "\n",
    "1. Recenter each feature of the dataset by subtracting that feature's mean from the feature vector\n",
    "2. Calculate the covariance matrix for your centered dataset\n",
    "3. Calculate the eigenvectors of the covariance matrix\n",
    "4. Project the dataset into the new feature space: Multiply the eigenvectors by the mean centered features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-unsupervised-learning-online-ds-pt-100719/master/images/pca.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions/Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">- **\"Decomposition\"**: breaking a matrix down into multiple matrices/vectors that can be combined again to produce the original matrix. \n",
    "    - There are many methods of decomposition, besides eigendecomposition. \n",
    "    - With time series we will discuss seasonal decomposition> breaking down a time series into seasonal components. \n",
    "\n",
    "\n",
    ">- **\"Eigendecomposition\"** will break down a matrix into 2 matrices: eigenvectors and eigenvalues.\n",
    "    - **\"Eigenvectors** are unit vectors, which means that their length or magnitude is equal to 1.0.\"*\n",
    "    - **\"Eigenvalues** are coefficients applied to eigenvectors that give the vectors their length or magnitude.\"*\n",
    "  \n",
    "_`*` = from: [Article: Gentle Introduction to Eigenvalues and Eigenvectors for Machine Learning]( https://machinelearningmastery.com/introduction-to-eigendecomposition-eigenvalues-and-eigenvectors/)_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ">- From Central Lecturer Notebook (updated since video recorded):\n",
    "    - \"Eigenvectors are related to eigenvalues by the following property: $\\vec{x}$ is an eigenvector of the matrix $A$ if $A\\vec{x} = \\lambda\\vec{x}$, for some eigenvalue $\\lambda$.\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"**Principal Components**\":\n",
    "    - The magnitude of the eigenvalue indicates how much variance that eigenvector captures/explains. \n",
    "    - The eigenvector that has explains the most variance in the data is called the \"First Principal Component\" or \"PC 1\".\n",
    "    - The eigenvector that explains the second-most variance after PC1 is PC2 or the second principal component. \n",
    "    \n",
    "- By selected the top X many principal components, we can capture the most variance in the data with the fewest number of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Use of PCA from My Neuroscience Research Days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_070620_FT_cohort_notes/master/images/Offline20Sorter.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTIVITY: USING PCA TO COLLAPSE 1800+ HEALTH FEATURES TO <10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:37:10.212617Z",
     "start_time": "2021-05-24T20:37:06.929356Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder,OneHotEncoder,MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer,MissingIndicator\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from ipywidgets import interact\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.templates.default='plotly_dark'\n",
    "\n",
    "np.random.seed(321)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pd.set_option('display.max_columns',0)\n",
    "pd.set_option('display.max_info_rows',200)\n",
    "plt.style.use('seaborn-notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data - NHANES (2013-2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/nhanes.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    ">The [National Health and Nutrition Examination Survey (NHANES)](https://www.cdc.gov/nchs/nhanes/about_nhanes.htm) is a program of continuous studies designed to assess the health and nutritional status of adults and children in the United States. The survey examines a nationally representative sample of about 5,000 persons located across the country each year. The survey is unique in that it combines interviews and physical examinations. The NHANES interview includes demographic, socioeconomic, dietary, and health-related questions. The examination component consists of medical, dental, and physiological measurements, as well as laboratory tests administered by highly trained medical personnel.\n",
    "\n",
    ">NHANES is a major program of the National Center for Health Statistics (NCHS). NCHS is part of the Centers for Disease Control and Prevention (CDC) and has the responsibility for producing vital and health statistics for the Nation.\n",
    "\n",
    "- *The Above was Borrowed (with Permission) from [Kristin's Phase 3 Project](https://github.com/kcoop610/phase-3-project)*\n",
    "\n",
    "\n",
    "#### LINKS:\n",
    "- [NHANES Dataset - Kaggle](https://www.kaggle.com/cdc/national-health-and-nutrition-examination-survey)\n",
    "\n",
    "- [Complete variable list](https://wwwn.cdc.gov/Nchs/Nhanes/Search/variablelist.aspx?Component=Demographics&CycleBeginYear=2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:28.547019Z",
     "start_time": "2021-05-24T20:13:28.541749Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys,glob\n",
    "folder = 'national-health-and-nutrition-examination-survey/'\n",
    "os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:28.552239Z",
     "start_time": "2021-05-24T20:13:28.548667Z"
    }
   },
   "outputs": [],
   "source": [
    "## Use glob to get list of csvs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:30.206698Z",
     "start_time": "2021-05-24T20:13:28.553627Z"
    }
   },
   "outputs": [],
   "source": [
    "## Load in all CSVs combined (one liner if you can...)\n",
    "# Here, it's files[1:] because of an invalid start byte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:30.360271Z",
     "start_time": "2021-05-24T20:13:30.208708Z"
    }
   },
   "outputs": [],
   "source": [
    "# So... what does this data look like?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Compress 1,800+ features down to 6 using PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First: need to explore and define our column groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:30.457375Z",
     "start_time": "2021-05-24T20:13:30.364658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some columns are mostly null data - let's explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:23:01.381184Z",
     "start_time": "2021-05-24T20:23:01.378744Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of mostly null columns\n",
    "high_null_cols = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:23:13.838109Z",
     "start_time": "2021-05-24T20:23:13.835712Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now a list of the rest of columns, which should all be numeric\n",
    "num_cols = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:23:15.792471Z",
     "start_time": "2021-05-24T20:23:15.790231Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get a list of categorical columns (that aren't mostly null)\n",
    "cat_cols = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:23:31.007191Z",
     "start_time": "2021-05-24T20:23:29.720212Z"
    }
   },
   "outputs": [],
   "source": [
    "# Explore those categorical columns's null values\n",
    "import missingno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:23:33.240007Z",
     "start_time": "2021-05-24T20:23:33.237746Z"
    }
   },
   "outputs": [],
   "source": [
    "## Check for null values in cat cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:23:37.245840Z",
     "start_time": "2021-05-24T20:23:37.243926Z"
    }
   },
   "outputs": [],
   "source": [
    "# Any of them have too many uniques to OHE?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:31.562899Z",
     "start_time": "2021-05-24T20:13:31.543597Z"
    }
   },
   "outputs": [],
   "source": [
    "## Verify we got all cols\n",
    "len([*num_cols, *cat_cols, *high_null_cols]) == len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:31.567096Z",
     "start_time": "2021-05-24T20:13:31.564785Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:31.632731Z",
     "start_time": "2021-05-24T20:13:31.569084Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's discuss - what steps am I doing? Why?\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('num_imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "ohe_transformer = Pipeline(steps=[\n",
    "    ('cat_imputer', SimpleImputer(strategy='constant', fill_value='MISSING')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "high_nulls_transformer = Pipeline(steps=[\n",
    "    ('null_indicator', MissingIndicator())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('cat_ohe', ohe_transformer, cat_cols), \n",
    "        ('cat_null', high_nulls_transformer, high_null_cols)])\n",
    "\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:24:36.022945Z",
     "start_time": "2021-05-24T20:24:36.020879Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply preprocessing to entire df and preview data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:24:42.012511Z",
     "start_time": "2021-05-24T20:24:42.010493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:34.389754Z",
     "start_time": "2021-05-24T20:13:34.387903Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make sure to grab the step, for explained variance later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:24:47.894502Z",
     "start_time": "2021-05-24T20:24:47.892633Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit transform with PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:37.855095Z",
     "start_time": "2021-05-24T20:13:37.851895Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's name these components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:37.869556Z",
     "start_time": "2021-05-24T20:13:37.856645Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add the column names and check out the PC data as a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:37.995162Z",
     "start_time": "2021-05-24T20:13:37.871169Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check how much variance is explained by all of our PCs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:37.999380Z",
     "start_time": "2021-05-24T20:13:37.996540Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How much of the total variance do these contain?\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what did we capture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:25:51.090755Z",
     "start_time": "2021-05-24T20:25:51.088999Z"
    }
   },
   "outputs": [],
   "source": [
    "### Plot PC1 vs PC2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:26:00.078864Z",
     "start_time": "2021-05-24T20:26:00.076623Z"
    }
   },
   "outputs": [],
   "source": [
    "## Turn it into a quick function\n",
    "def scatterplot_2D():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:26:09.157882Z",
     "start_time": "2021-05-24T20:26:09.155883Z"
    }
   },
   "outputs": [],
   "source": [
    "## plot pc2 vs pc3 with function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make an Interactive Function for Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:26:13.244855Z",
     "start_time": "2021-05-24T20:26:13.242870Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make interactive function to show any comparison\n",
    "from ipywidgets import interact\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are only visualizing a small portion of our PC data, lets add another dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make an interactive plotly scatter3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:26:37.017713Z",
     "start_time": "2021-05-24T20:26:37.015817Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_3D_PC():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What would we do with this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- Notice how there are groups of datapoints that seem to form groupings/clusters in 3-dimensional space. \n",
    "    - Next class we will use K-Means clustering to identify groups of people in our PC data.\n",
    "    - We will then try to explain those clusters using machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisiting Parkinson's Disease: Modeling with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- We previously discussed using feature selection methods to reduce the dimensionality of our Detecting Parkinson's via Speech Statistics dataset.\n",
    "    - Let's test using PCA to reduce dimensionality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Parkinsons' Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:39.338478Z",
     "start_time": "2021-05-24T20:13:39.291195Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "## Preprocessing tools\n",
    "from sklearn.model_selection import train_test_split,cross_val_predict,cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE,SMOTENC\n",
    "from sklearn import metrics\n",
    "\n",
    "## Models & Utils\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:39.833837Z",
     "start_time": "2021-05-24T20:13:39.339825Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import project_functions as pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:39.906846Z",
     "start_time": "2021-05-24T20:13:39.836120Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Changing Pandas Options to see full columns in previews and info\n",
    "n=800\n",
    "pd.set_option('display.max_columns',n)\n",
    "pd.set_option(\"display.max_info_rows\", n)\n",
    "pd.set_option('display.max_info_columns',n)\n",
    "pd.set_option('display.float_format',lambda x: f\"{x:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:42.235180Z",
     "start_time": "2021-05-24T20:13:39.909216Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/flatiron-school/Online-DS-FT-022221-Cohort-Notes/master/Phase_3/phase_3_project/feature_selection/pd_speech_features.csv',\n",
    "                 skiprows=1)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split & Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:42.518971Z",
     "start_time": "2021-05-24T20:13:42.236869Z"
    }
   },
   "outputs": [],
   "source": [
    "## Specifying root names of types of features to loop through and filter out from df\n",
    "target_col = 'class'\n",
    "drop_cols = ['id']\n",
    "\n",
    "## making gender a str so its caught by pipeline\n",
    "df['gender'] = df['gender'].astype(str)\n",
    "\n",
    "y = df[target_col].copy()\n",
    "X = df.drop(columns=[target_col,*drop_cols]).copy()\n",
    "\n",
    "\n",
    "## Train test split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=321)\n",
    "\n",
    "display(y_train.value_counts(),X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:42.945758Z",
     "start_time": "2021-05-24T20:13:42.520631Z"
    }
   },
   "outputs": [],
   "source": [
    "## saving list of numeric vs categorical feature\n",
    "num_cols = list(X_train.select_dtypes('number').columns)\n",
    "cat_cols = list(X_train.select_dtypes('object').columns)\n",
    "\n",
    "\n",
    "## create pipelines and column transformer\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer',SimpleImputer(strategy='median')),\n",
    "    ('scale',StandardScaler())\n",
    "])\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer',SimpleImputer(strategy='constant',fill_value='MISSING')),\n",
    "    ('encoder',OneHotEncoder(sparse=False,drop='if_binary'))])\n",
    "\n",
    "print('# of num_cols:',len(num_cols))\n",
    "print('# of cat_cols:',len(cat_cols))\n",
    "\n",
    "## COMBINE BOTH PIPELINES INTO ONE WITH COLUMN TRANSFORMER\n",
    "preprocessor=ColumnTransformer(transformers=[\n",
    "    ('num',num_transformer,num_cols),\n",
    "    ('cat',cat_transformer,cat_cols)])\n",
    "\n",
    "\n",
    "## Fit preprocessing pipeline on training data and pull out the feature names and X_cols\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "## Use the encoder's .get_feature_names\n",
    "cat_features = list(preprocessor.named_transformers_['cat'].named_steps['encoder']\\\n",
    "                            .get_feature_names(cat_cols))\n",
    "X_cols = num_cols+cat_features\n",
    "\n",
    "## Transform X_traian,X_test and remake dfs\n",
    "X_train_df = pd.DataFrame(preprocessor.transform(X_train),\n",
    "                          index=X_train.index, columns=X_cols)\n",
    "X_test_df = pd.DataFrame(preprocessor.transform(X_test),\n",
    "                          index=X_test.index, columns=X_cols)\n",
    "\n",
    "## Tranform X_train and X_test and make into DataFrames\n",
    "X_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:43.285323Z",
     "start_time": "2021-05-24T20:13:42.947229Z"
    }
   },
   "outputs": [],
   "source": [
    "## Save list of trues and falses for each cols\n",
    "smote_feats = [False]*len(num_cols) +[True]*len(cat_features)\n",
    "## resample training data\n",
    "smote = SMOTENC(smote_feats)\n",
    "X_train_sm,y_train_sm = smote.fit_resample(X_train_df,y_train)\n",
    "y_train_sm.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving `train_test_list` &  `train_test_list_sm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:43.336977Z",
     "start_time": "2021-05-24T20:13:43.287096Z"
    }
   },
   "outputs": [],
   "source": [
    "## saving train_test_list and train_test_list_sm\n",
    "train_test_list = None\n",
    "train_test_list_sm = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model with Original Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:27:25.505414Z",
     "start_time": "2021-05-24T20:27:25.503335Z"
    }
   },
   "outputs": [],
   "source": [
    "# Baseline linear RF  original features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:27:28.598980Z",
     "start_time": "2021-05-24T20:27:28.597020Z"
    }
   },
   "outputs": [],
   "source": [
    "# Baseline RF  original features - smote\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply PCA to reduce dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:45.821601Z",
     "start_time": "2021-05-24T20:13:45.759348Z"
    }
   },
   "outputs": [],
   "source": [
    "## Setting which version of data is used for PCA\n",
    "USE_RESAMPELD = False\n",
    "\n",
    "if USE_RESAMPELD:\n",
    "    X_tr,y_tr,X_te,y_te = train_test_list_sm\n",
    "    print('Using resampled data for PCA')\n",
    "else:\n",
    "    X_tr,y_tr,X_te,y_te = train_test_list\n",
    "    print('Using imbalanced data for PCA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:27:41.471058Z",
     "start_time": "2021-05-24T20:27:41.469036Z"
    }
   },
   "outputs": [],
   "source": [
    "## collapse 753 columns down to 10 with PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving `train_test_list_pca`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:27:45.459469Z",
     "start_time": "2021-05-24T20:27:45.457190Z"
    }
   },
   "outputs": [],
   "source": [
    "## save train_test_list_pca\n",
    "train_test_list_pca = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with PC Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:46.783678Z",
     "start_time": "2021-05-24T20:13:46.020115Z"
    }
   },
   "outputs": [],
   "source": [
    "## Fit and evaluate rf model with PC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Ideal # of Compoments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:46.842145Z",
     "start_time": "2021-05-24T20:13:46.785675Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:28:29.994265Z",
     "start_time": "2021-05-24T20:28:29.992174Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a pca_grid_pipe for testing n_components for our RF\n",
    "pca_grid_pipe = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:46.971789Z",
     "start_time": "2021-05-24T20:13:46.913092Z"
    }
   },
   "outputs": [],
   "source": [
    "## Let's try up to ~half the total number of features\n",
    "X_train_df.shape[1]//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:13:47.030384Z",
     "start_time": "2021-05-24T20:13:46.973655Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make n_components from 3 to half of total features (by 3s)\n",
    "n_components_list = list(range(3,X_train_df.shape[1]//2,3))\n",
    "print(len(n_components_list))\n",
    "n_components_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:29:13.048756Z",
     "start_time": "2021-05-24T20:29:13.046346Z"
    }
   },
   "outputs": [],
   "source": [
    "## define params grid for search (let's tune for f1_macro)\n",
    "params = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:29:17.567001Z",
     "start_time": "2021-05-24T20:29:17.564939Z"
    }
   },
   "outputs": [],
   "source": [
    "## Check the best params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:16:22.792811Z",
     "start_time": "2021-05-24T20:16:21.532356Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get the best pca pipeline from the gridsearch and evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Final Dataset & Model with Best `n_components` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:29:31.391705Z",
     "start_time": "2021-05-24T20:29:31.389572Z"
    }
   },
   "outputs": [],
   "source": [
    "## save best n_components \n",
    "best_n = None\n",
    "best_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:29:39.985297Z",
     "start_time": "2021-05-24T20:29:39.982730Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a final X_train_pca and X_test_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:18:29.370870Z",
     "start_time": "2021-05-24T20:18:29.317763Z"
    }
   },
   "outputs": [],
   "source": [
    "## save train_test_list_pca\n",
    "train_test_list_pca_final = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:29:59.812810Z",
     "start_time": "2021-05-24T20:29:59.811056Z"
    }
   },
   "outputs": [],
   "source": [
    "## Fit a random forest with the PC data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:30:05.037842Z",
     "start_time": "2021-05-24T20:30:05.035390Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get Built-in Importances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:30:16.696772Z",
     "start_time": "2021-05-24T20:30:15.919306Z"
    }
   },
   "outputs": [],
   "source": [
    "## Calculate permutation importance\n",
    "from sklearn.inspection import permutation_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:30:24.203353Z",
     "start_time": "2021-05-24T20:30:24.201091Z"
    }
   },
   "outputs": [],
   "source": [
    "## get list of features in order\n",
    "perm_important_features = None\n",
    "# perm_important_features[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Parkinsons PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:30:28.265975Z",
     "start_time": "2021-05-24T20:30:28.263967Z"
    }
   },
   "outputs": [],
   "source": [
    "## combine the X and y data as df_pca for visual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:20:08.585343Z",
     "start_time": "2021-05-24T20:20:08.530988Z"
    }
   },
   "outputs": [],
   "source": [
    "## Select sorting of PCs for visualization\n",
    "\n",
    "\n",
    "# features = list(X_train_pca.columns)\n",
    "# features = perm_importance_features\n",
    "# features = rf_important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T20:20:09.514569Z",
     "start_time": "2021-05-24T20:20:09.357277Z"
    }
   },
   "outputs": [],
   "source": [
    "@interact(x=features, y=features,z=features)\n",
    "def plot_3D_PC(x = features[0],\n",
    "               y = features[1],\n",
    "               z = features[2]):\n",
    "    ### Plot PC1 vs PC2\n",
    "    df_pca['class'] = df_pca['class'].astype(str)\n",
    "    pfig = px.scatter_3d(df_pca,x=x,y=y,z=z,color='class')\n",
    "    pfig.update_traces(marker={'size':2})\n",
    "    pfig.show(config = dict({'scrollZoom': False}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA is a dimensionality reduction technique that we can apply to our modeling process. \n",
    "    - PCA makes it difficult to interpret/understand which features are important."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env-new",
   "language": "python",
   "name": "learn-env-new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
