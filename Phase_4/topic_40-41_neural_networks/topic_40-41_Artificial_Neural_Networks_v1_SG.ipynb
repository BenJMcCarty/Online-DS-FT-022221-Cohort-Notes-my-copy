{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ByfUTZodQsND"
   },
   "source": [
    "# Topic 41-42: Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 06/04/21\n",
    "- onl91-dtsc-ft-022221"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start By Discussing Biological Neural Networks (powerpoint)\n",
    "- Connect back to introduction from Canvas\n",
    "- Demonstrate / play with Neural Network with Tensorflow Playground\n",
    "- Walk through making neural networks with tensorflow/keras. \n",
    "- Learn how to evaluate neural networks.\n",
    "\n",
    "- **Activity Part 1: Detecting Digits with Neural Networks**\n",
    "\n",
    "- **Activity Part 2: Detecting Digits with Deep Networks**\n",
    "- **Activity: Deeper Neural Networks Lab + Level Up**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions/ Comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ByfUTZodQsND"
   },
   "source": [
    "# Topic 41: Intro to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biological Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/Brainbow-Hippocampus-rainbow-colors-large.jpg\" width=60%>\n",
    "\n",
    "\n",
    "\n",
    "> See `intro_to_bio_neural_networks_v2.pptx` for introduction to how biological neurons work. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uceu7mnytDno"
   },
   "source": [
    "## Artificial Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "    \n",
    "- **The purpose of a neural network is to model $\\hat y \\approx y$ by minimizing loss/cost functions using gradient descent.**\n",
    "\n",
    "- Neural networks are very good with unstructured data. (images, audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "- **Networks are comprised of sequential layers of neurons/nodes.**\n",
    "    - Each neuron applies a **linear transformation** and an **activation function** and outputs its results to all neurons in the next layer.\n",
    "    - Minimizing Loss functions by adjusting parameters (weights and bias) of each connection using gradient descent (forward and back propagation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-introduction-to-neural-networks-online-ds-ft-100719/master/images/new_first_network_num.png\" width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-02-introduction-to-neural-networks-online-ds-ft-021119/master/figures/log_reg.png\">\n",
    "\n",
    "- **Linear transformations** ( $z = w^T x + b$ ) are used control the output of the activation function .\n",
    "    - where $w^T $ is the weight(/coefficient), $x$ is the input, and  $b$ is a bias. \n",
    "        - weights: \n",
    "        - bias:\n",
    "- **Activation functions** control the output of a neuron.($\\hat y =f_{activation}(x)$ )\n",
    "    - Most basic activation function is sigmoid functin ($\\hat y =\\sigma(x)$)\n",
    "    - Choice of activation function controls the size/range of the output.\n",
    "    \n",
    "    \n",
    "            \n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-02-introduction-to-neural-networks-online-ds-ft-021119/master/figures/log_reg_deriv.png\" >\n",
    "\n",
    "\n",
    "\n",
    "- **Loss functions** ($\\mathcal{L}(\\hat y, y) $)  measure inconsistency between predicted ($\\hat y$) and actual $y$\n",
    "    - will be optimized using gradient descent\n",
    "    - defined over 1 traning sample\n",
    "    \n",
    "    \n",
    "- **Cost functions** takes the average loss over all of the samples.\n",
    "    - $J(w,b) = \\displaystyle\\frac{1}{l}\\displaystyle\\sum^l_{i=1}\\mathcal{L}(\\hat y^{(i)}, y^{(i)})$\n",
    "    - where $l$ is the number of samples\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "### Activation Functions  *[ can return to later, if preferred]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The activation function introduced first in the curriculum is the sigmoid function. But there are many other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:50.810720Z",
     "start_time": "2021-06-04T15:21:49.991452Z"
    }
   },
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "- **sigmoid:**<br>\n",
    "<!-- <img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-04-deeper-neural-networks-online-ds-ft-021119/master/index_files/index_33_1.png\" width=200> -->\n",
    "    - $ f_a=\\dfrac{1}{1+ \\exp(-z)}$\n",
    "    - outputs 0 to +1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:51.041733Z",
     "start_time": "2021-06-04T15:21:50.812997Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_activation(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "- **tanh (hyperbolic tan):**<br>\n",
    "    - $f_a = =\\dfrac{\\exp(z)- \\exp(-z)}{\\exp(z)+ \\exp(-z)}$\n",
    "    - outputs -1 to +1\n",
    "    - Generally works well in intermediate layers\n",
    "    - one of most popular functions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:51.310540Z",
     "start_time": "2021-06-04T15:21:51.044488Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_activation(tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "- **arctan**\n",
    "    -  similar qualities as tanh, but slope is more gentle than tanh\n",
    "    - outputs ~ 1.6 to 1.6\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:51.562755Z",
     "start_time": "2021-06-04T15:21:51.313229Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_activation(arctan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "-  **Rectified Linear Unit (relu):**<br>\n",
    "    - most popular activation function\n",
    "    - Activation is exactly 0 when Z <0\n",
    "    - Makes taking directives slightly cumbersome\n",
    "    - $f_a=\\max(0,z)$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:51.806363Z",
     "start_time": "2021-06-04T15:21:51.565255Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_activation(relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "- **leaky_relu:**\n",
    "    -  altered version of relu where the activatiom is slightly negative when $z<0$\n",
    "    - $f_a=\\max(0.001*z,z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:52.042530Z",
     "start_time": "2021-06-04T15:21:51.808392Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_activation(leaky_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "### How ANNs \"Learn\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "- **Initialize the network with random starting weights and biases**\n",
    "\n",
    "\n",
    "- **Input the first batch** of training data to get first predictions, calculate the model's error/cost, and use gradient descent to update the weights and biases for the next batch. \n",
    "    - **Forward propagation** is the calculating  loss and cost functions.\n",
    "\n",
    "    - **Back propagation** involves using gradient descent to update the values for  $w$ and $b$.\n",
    "        - $w := w- \\alpha\\displaystyle \\frac{dJ(w)}{dw}$ <br><br>\n",
    "        - $b := b- \\alpha\\displaystyle \\frac{dJ(b)}{db}$\n",
    "\n",
    "        - where $ \\displaystyle \\frac{dJ(w)}{dw}$ and $\\displaystyle \\frac{dJ(b)}{db}$ represent the *slope* of the function $J$ with respect to $w$ and $b$ respectively\n",
    "        - $\\alpha$ denote the *learning rate*. \n",
    "- After updating the weights and biases, **repeat the process for each batch of data until the entire training dataset** has been used. \n",
    "\n",
    "- The process described above is **1 epoch**. An epoch is when the neural network has seen all batches of the training data. \n",
    "        \n",
    "<!--         \n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/images/neural_network_steps.png\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Tensorflow Playground \n",
    "- Open the task linked below and visualize the intuition behind the processes described above. \n",
    "\n",
    "- [Tensorflow Playground Task](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.70857&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JkJpdZlv3VNe"
   },
   "source": [
    "# Intro to Tensorflow/Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensorflow Links\n",
    "    - https://www.tensorflow.org/\n",
    "    -https://www.tensorflow.org/api_docs/python/tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Keras Note: **Do not use the standalone keras package** nowadays, it is better to use the **keras submodule** in tensorflow 2.x\n",
    "\n",
    "\n",
    "> **Note re Random Seeds:** Neural Networks have a LOT of randomized processed. Fixing the results of ANNs involves using tf.random.set_seed AND np.random.seed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:55.175916Z",
     "start_time": "2021-06-04T15:21:52.045189Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -U tensorflow\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(321)\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bu64jh4K3oKt"
   },
   "source": [
    "## Overview of Building a Neural Network with Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNk4Uz27wVZc"
   },
   "source": [
    "**Basics of Building a Neural Network with Keras:**\n",
    "1. **Import required modules**\n",
    "    - **For general neural network**\n",
    "        - `from tensorflow.keras import models, layers,optimizers`\n",
    "    - **For text:**\n",
    "        - `from tensorflow.keras.preprocessing.text import Tokenizer`\n",
    "        - `from tensorflow.keras.utils import to_categorical`\n",
    "    - **For images:**\n",
    "        - `from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, array_to_img`\n",
    "    - **For relocating image files:**\n",
    "        - `import os, shutil`\n",
    "\n",
    "\n",
    "\n",
    "2. **Decide on a network architecture (have only discussed sequential thus far)**\n",
    "    - `model = models.Sequential()`\n",
    "\n",
    "\n",
    "\n",
    "3. **Adding layers - specifying layer type, number of neurons, activation functions, and, optionally, the input shape.**\n",
    "    - `model.add(layers.Dense(units, activation='relu', input_shape))`\n",
    "    - `model.add(layers.Dense(units, activation='relu',input_shape))`\n",
    "    - **3B. Final layer choice:**\n",
    "        - Want to have as many neurons as classes you are trying to predict\n",
    "        -  Final activation function:\n",
    "            - For binary classificaiton, use `activation='sigmoid'`\n",
    "            - For multi classificaiton, use `activation='softmax'`\n",
    "        - For regression tasks, have a single final neuron.\n",
    "\n",
    "\n",
    "\n",
    "4. **Compile the model:**\n",
    "    - Specify optimiziers\n",
    "        - `RMSprop`, `SGD`\n",
    "    - specify loss functions\n",
    "        - for binary classification: `'binary_crossentropy'`\n",
    "        - for multi classification: `'categorical_crossentropy'`\n",
    "    - specify metrics\n",
    "        -usually 'acc'\n",
    "    \n",
    "    \n",
    "\n",
    "5. **Training the model**\n",
    "    - `model.fit(X_train, y_train, epochs=20,batch_size=512,validation_data=(x_val,y_val))`\n",
    "        - Note: if using images with ImageDataGenerator, use `model.fit_generator()`\n",
    "    \n",
    "    - **batches:**\n",
    "        - a set of N samples, processed independently in parallel\n",
    "        - a batch determines how many samples are fed through before back-propagation. \n",
    "        - model only updates after a batch is complete.\n",
    "        - ideally have as large of a batch as your hardware can handle without going out of memory.\n",
    "            - larger batches usually run faster than smaller ones for evaluation/prediction. \n",
    "    - **epoch:**\n",
    "        - arbitrary cutoff / \"one pass over the entire dataset\", useful for logging and periodic evaluation\n",
    "        - when using kera's `model.fit` parameters `validation_data` or `validation_split`, these evaluations run at the end of every epoch.\n",
    "        - Within Keras can add callbacksto be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving).\n",
    "        \n",
    "\n",
    "6. **Evaluation / Predictions**\n",
    "    - To get predicted results:\n",
    "        - `y_hat_test = model.predict(test)`\n",
    "    - To get evaluation metrics:\n",
    "        - `results_test = model.evaluate(test, label_test)`\n",
    "        \n",
    "\n",
    "7. **Visualization**\n",
    "    - **`history =  model.fit()` creates history object with .history attribute.**\n",
    "        - `history.history()` returns a dictionary of metrics from each epoch. \n",
    "            - `history.history['loss']` and `history.history['acc']` \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Activity Part 1: Detecting Digits with Shallow Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNk4Uz27wVZc"
   },
   "source": [
    "**Basics of Building a Neural Network with Keras:**\n",
    "1. Import required modules\n",
    "2. Decide on a network architecture (have only discussed sequential thus far)\n",
    "3. Adding layers - specifying layer type, number of neurons, activation functions, and, optionally, the input shape.\n",
    "4. Compile the model:\n",
    "    - Specify optimiziers\n",
    "    - specify loss functions\n",
    "    - specify metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:55.186340Z",
     "start_time": "2021-06-04T15:21:55.182294Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "NbbVDhN0rDhV",
    "outputId": "4b5f722d-e72f-42bd-905b-b965aee215f8"
   },
   "outputs": [],
   "source": [
    "## Tensorflow imports\n",
    "# !pip install -U tensorflow\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(321)\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import layers,optimizers,activations,models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:55.662345Z",
     "start_time": "2021-06-04T15:21:55.189756Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "NbbVDhN0rDhV",
    "outputId": "4b5f722d-e72f-42bd-905b-b965aee215f8"
   },
   "outputs": [],
   "source": [
    "## Import the usual suspects (notice the mpl import)\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "# %matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:56.040245Z",
     "start_time": "2021-06-04T15:21:55.664122Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "NbbVDhN0rDhV",
    "outputId": "4b5f722d-e72f-42bd-905b-b965aee215f8"
   },
   "outputs": [],
   "source": [
    "## Load in the mnist dataset from tensorflow\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "digit = train_images[10] #Select an arbitrary case for our example\n",
    "\n",
    "#Checking the shape of our tensor (in this case, the image)\n",
    "print('Raw Tensor shape:', train_images.shape)\n",
    "\n",
    "#Now performing some slices of our image:\n",
    "print('Single Image shape', train_images[0].shape)\n",
    "\n",
    "\n",
    "plt.imshow(digit, cmap=plt.cm.binary) #Display an example image for context\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:56.046625Z",
     "start_time": "2021-06-04T15:21:56.042433Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_images), len(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYecj1xvmvEU"
   },
   "source": [
    "#### Tensor Basics\n",
    "\n",
    "- Tensors dimensions:\n",
    "    - Scalars = 0D tensors\n",
    "    - Vectors = 1D tensors\n",
    "    - Matrices = 2D tensors\n",
    "    - 3D tensors\n",
    "- A tensor is defined by 3 characteristics:\n",
    "    - rank or number of axes\n",
    "    - the shape\n",
    "    - the data type\n",
    "- Tensor basics - properties (from [here](https://pgaleone.eu/tensorflow/2018/07/28/understanding-tensorflow-tensors-shape-static-dynamic/#tensors-the-basic)):\n",
    "    - name\n",
    "    - type:\n",
    "        - tf.float32, tf.int64, tf.string\n",
    "    - rank:\n",
    "        - the number of dimension or the tensor. \n",
    "        - scalar = 0, vector = 1, etc.\n",
    "    - shape:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYecj1xvmvEU"
   },
   "source": [
    "#### Important Data Manipulations in numpy\n",
    "\n",
    "- **Unrowing matrices:**\n",
    "    - e.g. turning a matrix of 790 images, which are 64 x 64 pixels and in RBG (3 colors) a (790, 64, 64, 3) matrix  into a matrix with 1 row for each image a ( 64*64*3, 790) matrix\n",
    "    - img_unrow = img.reshape(790, -1).T\n",
    "        - reshape -1 essentially means \"figure out how many, based upon the dat'\n",
    "- **Increasing the rank:**\n",
    "    - Vector with `np.shape()` returns  `(790,)`\n",
    "    - `np.reshape(vector, (1, 790))`\n",
    "- **Tensor indexling/slicing**\n",
    "    - just as python, `tensor[start_idx : end_idx]`\n",
    "    - left inclusive, right exclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:56.330031Z",
     "start_time": "2021-06-04T15:21:56.048611Z"
    }
   },
   "outputs": [],
   "source": [
    "## Reshape images as (num_images,-1)\n",
    "\n",
    "\n",
    "## Scale data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:56.337265Z",
     "start_time": "2021-06-04T15:21:56.332150Z"
    }
   },
   "outputs": [],
   "source": [
    "## check train_img_unrow shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:56.343640Z",
     "start_time": "2021-06-04T15:21:56.339844Z"
    }
   },
   "outputs": [],
   "source": [
    "## check out an individual array's shape from train_img_unrow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Targets\n",
    "\n",
    "- This is a **Multiclass Classification** problem.\n",
    "    - we need to One-Hot Encode our labels\n",
    "    - `tensorflow.keras.utils.to_categorical`\n",
    "    \n",
    "- For multi classification:\n",
    "    - good final activation function is softmax\n",
    "    - categorical_crossentytropu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:56.351830Z",
     "start_time": "2021-06-04T15:21:56.345781Z"
    }
   },
   "outputs": [],
   "source": [
    "## Prepare y_train and y_train using to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:56.356961Z",
     "start_time": "2021-06-04T15:21:56.354025Z"
    }
   },
   "outputs": [],
   "source": [
    "## Check the shape of y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:56.363704Z",
     "start_time": "2021-06-04T15:21:56.359780Z"
    }
   },
   "outputs": [],
   "source": [
    "## check the first row of y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "58e3B6PDzIjZ"
   },
   "source": [
    "#### 5. Training the model\n",
    "- `model.fit(X_train, y_train, epochs=20,batch_size=512,validation_data=(x_val,y_val))`\n",
    "\n",
    "- **batches:**\n",
    "    - a set of N samples, processed independently in parallel\n",
    "    - a batch determines how many samples are fed through before back-propagation. \n",
    "    - model only updates after a batch is complete.\n",
    "    - ideally have as large of a batch as your hardware can handle without going out of memory.\n",
    "        - larger batches usually run faster than smaller ones for evaluation/prediction. \n",
    "- **epoch:**\n",
    "    - arbitrary cutoff / \"one pass over the entire dataset\", useful for logging and periodic evaluation\n",
    "    - when using kera's `model.fit` parameters `validation_data` or `validation_split`, these evaluations run at the end of every epoch.\n",
    "    - Within Keras can add callbacksto be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving).\n",
    "\n",
    "- **`history =  model.fit()` creates history object with .history attribute.**\n",
    "    - `history.history()` returns a dictionary of metrics from each epoch. \n",
    "        - `history.history['loss']` and `history.history['acc']`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Network Architecture & Fitting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Where tensor/matrix shapes will come back to haunt us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:56.371486Z",
     "start_time": "2021-06-04T15:21:56.365813Z"
    }
   },
   "outputs": [],
   "source": [
    "## Print y_train shape\n",
    "\n",
    "\n",
    "## Save the number of cols as n_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:56.376810Z",
     "start_time": "2021-06-04T15:21:56.373937Z"
    }
   },
   "outputs": [],
   "source": [
    "## What is the shape of our individual images?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:21:56.423868Z",
     "start_time": "2021-06-04T15:21:56.378904Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Ary82Ge-wfTb"
   },
   "outputs": [],
   "source": [
    "## Make a Sequential Model\n",
    "\n",
    "## Hidden layer of 10 units with relu activation function\n",
    "\n",
    "## Add final layer with n_classes # of neurons and activation='softmax'\n",
    "\n",
    "## Compile with optimer as adam, loss as categorical_crossentropy, and accuracy as metric\n",
    "\n",
    "\n",
    "## Display model summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test-Val Splits\n",
    "- Starting with ANNs, we will want to have 3 splits for our data. \n",
    "    - Training Set: bulk/majority of the data (~70-80% of data)\n",
    "    - Test Set: ~15-20% of data.\n",
    "    - Validation Set: ~5-10% of data.\n",
    "    \n",
    "- **The training and test sets are used the same way** we used to with machine learning models.\n",
    "    - Fit the model with the training data.\n",
    "    - Evaluate the model's ability to generalize to new data withthe test split.\n",
    "    \n",
    ">- The Validation split is used to test for overfitting **during the training process**\n",
    "\n",
    "- Note: in most situations we can have keras automatically take a portion fo the training data to use as validation data. \n",
    "`model.fit(X_train,y_train,validation_split=...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:22:06.892214Z",
     "start_time": "2021-06-04T15:21:56.426667Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Ary82Ge-wfTb"
   },
   "outputs": [],
   "source": [
    "# Fitting the model (10 epochs, batch size=64, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:22:06.901976Z",
     "start_time": "2021-06-04T15:22:06.893460Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a dataframe out of history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:22:07.300264Z",
     "start_time": "2021-06-04T15:22:06.906924Z"
    }
   },
   "outputs": [],
   "source": [
    "## Plot each type of metric on its own plot (accuracy/loss)\n",
    "\n",
    "## Plot Train/Val Losses\n",
    "\n",
    "## Plot Train/Val accuracy\n",
    "\n",
    "\n",
    "## Set the xticks to be 1 tick for each epoch  and add a grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Q: What are we looking at? What should we check for?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:22:09.209209Z",
     "start_time": "2021-06-04T15:22:07.303052Z"
    }
   },
   "outputs": [],
   "source": [
    "## Use model.evalaute to get train and test accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To use sklearn's metrics functions, we will need to get predictions for our data, per usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:22:10.444011Z",
     "start_time": "2021-06-04T15:22:09.210732Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get predictions for train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:22:19.710922Z",
     "start_time": "2021-06-04T15:22:19.684769Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print Classification Report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Ruh roh! What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:22:29.689165Z",
     "start_time": "2021-06-04T15:22:29.685809Z"
    }
   },
   "outputs": [],
   "source": [
    "## Check y_test, and y_hat_test's shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:22:29.696292Z",
     "start_time": "2021-06-04T15:22:29.692657Z"
    }
   },
   "outputs": [],
   "source": [
    "## What does one y-value look like?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:22:29.701601Z",
     "start_time": "2021-06-04T15:22:29.698404Z"
    }
   },
   "outputs": [],
   "source": [
    "## What is one y-value's shape?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural networks return the same # of values as the final layer of neurons. \n",
    "- we have 10 classes, one neuron for each of our classes (digits 0-9)\n",
    "- since we used `softmax` as our final layer's activation function, we get **probabilities** of falling into **each of the 10 classes.**\n",
    "\n",
    "\n",
    "- We want to use the class with the highest probability as our final prediction..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:22:29.707493Z",
     "start_time": "2021-06-04T15:22:29.703608Z"
    }
   },
   "outputs": [],
   "source": [
    "## Take the .argmax(axis=1) of the y-data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:22:29.716106Z",
     "start_time": "2021-06-04T15:22:29.709490Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make y_pred and y_true and y_test sklearn compatible\n",
    "\n",
    "## Make y_pred and y_true and y_test sklearn compatible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:29:07.960356Z",
     "start_time": "2021-06-04T15:29:07.956879Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the classification report for trianing and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:22:30.925021Z",
     "start_time": "2021-06-04T15:22:29.856329Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get the confusion matrix \n",
    "\n",
    "## Plot the confusion Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def `evaluate_network`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Combine all of the steps above into an evaluate netowrk function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:29:50.440668Z",
     "start_time": "2021-06-04T15:29:50.437064Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_network(model, X_test,y_test,history=None,\n",
    "                     X_train = None, y_train = None,\n",
    "                     cmap='Greens', normalize='true',\n",
    "                     classes=None,figsize=(10,4), \n",
    "                     metrics_list=['acc','loss']):\n",
    "    \"\"\"Evaluates a scikit-learn binary classification model.\n",
    "\n",
    "    Args:\n",
    "        model (classifier): a fit keras classification model.\n",
    "        X_test (tensor/array): X data\n",
    "        y_test (Tensor/Array): y data\n",
    "        history (History object): model history from .fit\n",
    "        cmap (str, optional): Colormap for confusion matrix. Defaults to 'Greens'.\n",
    "        normalize (str, optional): normalize argument for plot_confusion_matrix. \n",
    "                                    Defaults to 'true'.\n",
    "        classes (list, optional): List of class names for display. Defaults to None.\n",
    "        figsize (tuple, optional): figure size Defaults to (8,4).\n",
    "        \n",
    "        X_train (Frame or Array, optional): If provided, compare model.score \n",
    "                                for train and test. Defaults to None.\n",
    "        y_train (Series or Array, optional): If provided, compare model.score \n",
    "                                for train and test. Defaults to None.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    ## First, Plot History, if provided.\n",
    "   \n",
    "           \n",
    "        \n",
    "    ## Evaluate Network for loss/acc scores\n",
    "  \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "     ## Evaluate training data with sklearn\n",
    " \n",
    "        \n",
    "    ## Test Data - Print report header, get preds, get class report, and conf matrix\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:29:53.897630Z",
     "start_time": "2021-06-04T15:29:53.895827Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Evaliuate our model with our function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Great! Now we have an easy way to compare our networks as we continue on to Deep Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 41:  Deeper Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-introduction-to-neural-networks-onl01-dtsc-ft-070620/master/images/Deeper_network.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Deeper Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a \"deep\" neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Any network with more than 1 hidden layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why deeper networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Advantages:**\n",
    "    - largely eliminates need for feature engineering\n",
    "    - multiple levels of information processing in one networking.\n",
    "        - Ex: for images:\n",
    "            - First layer detects edges\n",
    "            - second layer gorups edges and detects patterns\n",
    "            - more layers group even bigger parts together\n",
    "        - Ex: for audio:\n",
    "            - first layer: low level wave features\n",
    "            - second: basic units of sounds (\"phonemes\")\n",
    "            - third: word recognition\n",
    "            - fourth: sentence recognition\n",
    "          \n",
    "          \n",
    "- **Disadvantages**:\n",
    "    - Longer training time\n",
    "    - Overfits very easily.\n",
    "            \n",
    "> #### Revisit [Tensorflow Playground with more complex task](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.42037&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many layers/units?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Two schools of thought on how how many layers:**\n",
    "    - Start with a single layer with few neurons\n",
    "        - Add additional, add additional\n",
    "    - Start with a fully fleshed out network that we then prune until we see a dropoff in in performance (then restore the last changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Activity Part 2: Detecting Digits with Deep Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reat our modeling steps from above, but wrap it in a `make_deep_model` function and increase the number of layers and re-evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:30:01.843482Z",
     "start_time": "2021-06-04T15:30:01.841359Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_deep_model():\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:30:05.247010Z",
     "start_time": "2021-06-04T15:30:05.245151Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use our build function to make a deep model, fit it, and evaluate it with our func\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: How did our deeper model do? What are we noticing? What to try next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Next Class: Tuning Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u2NJkNCFN9kQ"
   },
   "source": [
    "## **Activity: Deeper Neural Networks Lab + Level Up**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Bonus Assignment for us: Add a final model where we use MinMaxScaler instead of StandardScaler and look for convergence.**\n",
    "- `repo folder` > `Phase_4` >`topic_40-41_neural_networks` >`labs`>`dsc-deeper-neural-networks-lab-master`> `index_level_up-SG.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### A Note On Shapes\n",
    "\n",
    "- Inputs:\n",
    "    - $n$: Number of inputs (columns) in the feature vector \n",
    "    - $l$: Number of items (rows) in the training set \n",
    "    - $m$: Number of items (rows) in the test set\n",
    "    \n",
    "    \n",
    "- Input X:\n",
    "    - Will have shape $n$ x $l$ (number of features x number of training data points/rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## ANN Visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- https://github.com/RedaOps/ann-visualizer\n",
    "    - HAVE NOT TESTED FOR COMPATIBILITY WITH LEARN-ENV YET\n",
    "    - requires graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "### Using the chain rule for updating parameters with sigmoid activation function example:\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-02-introduction-to-neural-networks-online-ds-ft-021119/master/figures/log_reg_deriv.png\" >\n",
    "- $\\displaystyle \\frac{dJ(w,b)}{dw_i} = \\displaystyle\\frac{1}{l}\\displaystyle\\sum^l_{i=1} \\frac{d\\mathcal{L}(\\hat y^{(i)}, y^{(i)})}{dw_i}$\n",
    " \n",
    " \n",
    "- For each training sample $1,...,l$ you'll need to compute:\n",
    "\n",
    "    - $ z^{(i)} = w^T x^ {(i)} +b $\n",
    "\n",
    "    - $\\hat y^{(i)} = \\sigma (z^{(i)})$\n",
    "\n",
    "    - $dz^{(i)} = \\hat y^{(i)}- y^{(i)}$\n",
    "\n",
    "- Then, you'll need to make update:\n",
    "\n",
    "    - $J_{+1} = - [y^{(i)} \\log (\\hat y^{(i)}) + (1-y^{(i)}) \\log(1-\\hat y^{(i)})$ (for the sigmoid function)\n",
    "\n",
    "    - $dw_{1, +1}^{(i)} = x_1^{(i)} * dz^{(i)}$\n",
    "\n",
    "    - $dw_{2, +1}^{(i)} = x_2^{(i)} * dz^{(i)}$\n",
    "\n",
    "    - $db_{+1}^{(i)} =  dz^{(i)}$\n",
    "\n",
    "    - $\\dfrac{J}{m}$, $\\dfrac{dw_1}{m}$, $\\dfrac{dw_1}{m}$, $\\dfrac{db}{m}$\n",
    "\n",
    "- After that, update: \n",
    "\n",
    "    $w_1 := w_1 - \\alpha dw_1$\n",
    "\n",
    "    $w_2 := w_2 - \\alpha dw_2$\n",
    "\n",
    "    $b := b - \\alpha db$\n",
    "\n",
    "    repeat until convergence!\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "My Flatiron Bootcamp Notes - Mod 4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "learn-env-new",
   "language": "python",
   "name": "learn-env-new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "325px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
