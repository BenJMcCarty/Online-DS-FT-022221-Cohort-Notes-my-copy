{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 39: Natural Language Processing + Topic 28: Bayesian Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 06/03/21\n",
    "- onl01-dtsc-ft-022221"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Introduce the field of Natural Language Processing\n",
    "- Learn about the extensive preprocessing involved with text data\n",
    "- Walk through text classification - Finding Trump \n",
    "- Discuss how to use GridSearchCV for our text preprocessing/vectorization choices. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`Part 1` Topic 39: Natural Language Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Natural Language Processing_**, or **_NLP_**, is the study of how computers can interact with humans through the use of human language.  Although this is a field that is quite important to Data Scientists, it does not belong to Data Science alone.  NLP has been around for quite a while, and sits at the intersection of *Computer Science*, *Artificial Intelligence*, *Linguistics*, and *Information Theory*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where is NLP Used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Reviews (i.e. Amazon)\n",
    "- AI Assistants - [Google Duplex AI Assistant](https://youtu.be/D5VN56jQMWM)\n",
    "- Spam Detection\n",
    "- Stock market trading - [Volvfefe Index]( https://www.bloomberg.com/news/articles/2019-09-09/jpmorgan-creates-volfefe-index-to-track-trump-tweet-impact)\n",
    "- Chatbots/Text Generation [GPT2 Blog Post](https://openai.com/blog/better-language-models/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare text data for modeling, **it is essential to preprocess the text** and simplify its contents.\n",
    "<br><br>\n",
    "1. **At a minimum, things like:**\n",
    "    - punctuation\n",
    "    - numbers\n",
    "    - upper vs lowercase letters<br>\n",
    "    ***must*** be addressed before any initial analyses. \n",
    "\n",
    "\n",
    "\n",
    "2. It is **usually recommended** to remove **commonly used words that contain little information** <br>for our machine learning algorithms. Words like: (the,was,he,she, it,etc.)<br> are called **\"stopwords\"**, and it is critical to address them as well.\n",
    "\n",
    "\n",
    "3. Our text will need to be separated in a list of words, AKA **tokenized**.\n",
    "    - A list of words (**tokens**) separated by \",\", which tells the algorithm what should be considered one word.\n",
    "    - There are different methods/tools for tokenizing the data, which can make a big difference on the resulting text.\n",
    "    \n",
    "    \n",
    "    \n",
    "4. While not always required, it is often a good idea to reduce similar words down to a shared core.\n",
    "    - There are often **multiple variants of the same word with the same/simiar meaning**, but one may plural **(i.e. \"democrat\" and \"democrats\")**, or form of words is different **(i.e. run, running).** \n",
    "    - Simplifying words down to the basic core word (or word *stem*) is referred to as **\"stemming\"**. <br><img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-nlp-and-word-vectorization-online-ds-ft-100719/master/images/new_stemming.png\" width=40%>\n",
    "    \n",
    "    - A more advanced form of this also understands things like words that are just in a **different tense** such as  i.e.  **\"ran\", \"run\", \"running\"**. This process is called  **\"lemmatization**, where the words are reduced to their simplest form, called \"**lemmas**\"\n",
    "     \n",
    "|   Word   |  Stem | Lemma |\n",
    "|:--------:|:-----:|:-----:|\n",
    "|  Studies | Studi | Study |\n",
    "| Studying | Study | Study |\n",
    "\n",
    "5. Finally, we have to convert our text data into numeric form for our machine learning models to analyze, a process called **vectorization**.\n",
    "    - There are also several options for vectorization, which can also have a great impact on the resulting model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Preprocessing Text with Trump's Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:59:45.982503Z",
     "start_time": "2021-06-03T15:59:44.527611Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "\n",
    "## NLP Imports\n",
    "import nltk\n",
    "from nltk import FreqDist,word_tokenize,regexp_tokenize,TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# !pip install wordcloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:59:46.059035Z",
     "start_time": "2021-06-03T15:59:45.984779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter Media Studio</td>\n",
       "      <td>https://t.co/EVAEYD1AgV</td>\n",
       "      <td>01-01-2020 03:12:07</td>\n",
       "      <td>25016</td>\n",
       "      <td>108830</td>\n",
       "      <td>False</td>\n",
       "      <td>1212209862094012416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>HAPPY NEW YEAR!</td>\n",
       "      <td>01-01-2020 01:30:35</td>\n",
       "      <td>85409</td>\n",
       "      <td>576045</td>\n",
       "      <td>False</td>\n",
       "      <td>1212184310389850119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Our fantastic First Lady! https://t.co/6iswto4WDI</td>\n",
       "      <td>01-01-2020 01:22:28</td>\n",
       "      <td>27567</td>\n",
       "      <td>132633</td>\n",
       "      <td>False</td>\n",
       "      <td>1212182267113680896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @DanScavino: https://t.co/CJRPySkF1Z</td>\n",
       "      <td>01-01-2020 01:18:47</td>\n",
       "      <td>10796</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1212181341078458369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @SenJohnKennedy: I think Speaker Pelosi is ...</td>\n",
       "      <td>01-01-2020 01:17:43</td>\n",
       "      <td>8893</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1212181071988703232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14061</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>The President of Taiwan CALLED ME today to wis...</td>\n",
       "      <td>12-03-2016 00:44:20</td>\n",
       "      <td>24700</td>\n",
       "      <td>111106</td>\n",
       "      <td>False</td>\n",
       "      <td>804848711599882240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14062</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Thank you Ohio! Together we made history – and...</td>\n",
       "      <td>12-02-2016 02:45:18</td>\n",
       "      <td>17283</td>\n",
       "      <td>72196</td>\n",
       "      <td>False</td>\n",
       "      <td>804516764562374656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14063</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Heading to U.S. Bank Arena in Cincinnati Ohio ...</td>\n",
       "      <td>12-01-2016 22:52:10</td>\n",
       "      <td>5564</td>\n",
       "      <td>31256</td>\n",
       "      <td>False</td>\n",
       "      <td>804458095569158144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14064</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Getting ready to leave for the Great State of ...</td>\n",
       "      <td>12-01-2016 14:38:09</td>\n",
       "      <td>9834</td>\n",
       "      <td>57249</td>\n",
       "      <td>False</td>\n",
       "      <td>804333771021570048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14065</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>My thoughts and prayers are with those affecte...</td>\n",
       "      <td>12-01-2016 14:37:57</td>\n",
       "      <td>12077</td>\n",
       "      <td>65724</td>\n",
       "      <td>False</td>\n",
       "      <td>804333718999539712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14066 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     source  \\\n",
       "0      Twitter Media Studio   \n",
       "1        Twitter for iPhone   \n",
       "2        Twitter for iPhone   \n",
       "3        Twitter for iPhone   \n",
       "4        Twitter for iPhone   \n",
       "...                     ...   \n",
       "14061   Twitter for Android   \n",
       "14062    Twitter for iPhone   \n",
       "14063    Twitter for iPhone   \n",
       "14064   Twitter for Android   \n",
       "14065    Twitter for iPhone   \n",
       "\n",
       "                                                    text           created_at  \\\n",
       "0                                https://t.co/EVAEYD1AgV  01-01-2020 03:12:07   \n",
       "1                                        HAPPY NEW YEAR!  01-01-2020 01:30:35   \n",
       "2      Our fantastic First Lady! https://t.co/6iswto4WDI  01-01-2020 01:22:28   \n",
       "3                RT @DanScavino: https://t.co/CJRPySkF1Z  01-01-2020 01:18:47   \n",
       "4      RT @SenJohnKennedy: I think Speaker Pelosi is ...  01-01-2020 01:17:43   \n",
       "...                                                  ...                  ...   \n",
       "14061  The President of Taiwan CALLED ME today to wis...  12-03-2016 00:44:20   \n",
       "14062  Thank you Ohio! Together we made history – and...  12-02-2016 02:45:18   \n",
       "14063  Heading to U.S. Bank Arena in Cincinnati Ohio ...  12-01-2016 22:52:10   \n",
       "14064  Getting ready to leave for the Great State of ...  12-01-2016 14:38:09   \n",
       "14065  My thoughts and prayers are with those affecte...  12-01-2016 14:37:57   \n",
       "\n",
       "       retweet_count  favorite_count is_retweet               id_str  \n",
       "0              25016          108830      False  1212209862094012416  \n",
       "1              85409          576045      False  1212184310389850119  \n",
       "2              27567          132633      False  1212182267113680896  \n",
       "3              10796               0       True  1212181341078458369  \n",
       "4               8893               0       True  1212181071988703232  \n",
       "...              ...             ...        ...                  ...  \n",
       "14061          24700          111106      False   804848711599882240  \n",
       "14062          17283           72196      False   804516764562374656  \n",
       "14063           5564           31256      False   804458095569158144  \n",
       "14064           9834           57249      False   804333771021570048  \n",
       "14065          12077           65724      False   804333718999539712  \n",
       "\n",
       "[14066 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load in the finding-trump.csv\n",
    "finding_trump = 'finding-trump.csv'\n",
    "df = pd.read_csv(finding_trump)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:08.249351Z",
     "start_time": "2021-06-03T15:28:08.245852Z"
    }
   },
   "outputs": [],
   "source": [
    "## Create a variable \"corpus\" containing all text\n",
    "corpus = df['text'].to_list()\n",
    "\n",
    "## Preview first 5 entries \n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Bag-of-Words Frequency Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"bag-of-words\": collection of all words from a corpus and their frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:09.160757Z",
     "start_time": "2021-06-03T15:28:08.250757Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:10.174790Z",
     "start_time": "2021-06-03T15:28:09.162313Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a FreqDist from the corpus\n",
    "freq = FreqDist(','.join(corpus))\n",
    "## Display 100 most common words\n",
    "freq.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> That's not quite right..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:12.585401Z",
     "start_time": "2021-06-03T15:28:10.176698Z"
    }
   },
   "outputs": [],
   "source": [
    "## Tokenize corpus then generate FreqDist\n",
    "from nltk import word_tokenize\n",
    "\n",
    "## Convert Corpus to Tokens\n",
    "tokens = word_tokenize(','.join(corpus))\n",
    "\n",
    "## Check first 5 tokens\n",
    "print(tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:13.321626Z",
     "start_time": "2021-06-03T15:28:12.591205Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get FreqDist and plot the 25 most_common tokens\n",
    "freq = FreqDist(tokens)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "freq.plot(25);\n",
    "\n",
    "## Rotate \n",
    "ax.set_xticklabels(ax.get_xticklabels(),\n",
    "                   rotation=45,ha='right');\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:13.675577Z",
     "start_time": "2021-06-03T15:28:13.325223Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get the most_common 100 and make into a dataframe\n",
    "most_common = pd.DataFrame(freq.most_common(100),\n",
    "                           columns=['word','count']).sort_values('count',\n",
    "                                                                 ascending=True)\n",
    "most_common.set_index('word').tail(25).plot(kind='barh',figsize=(12,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:13.954090Z",
     "start_time": "2021-06-03T15:28:13.677555Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_most_common(freq,n=25,figsize=(12,5)):\n",
    "    most_common = pd.DataFrame(freq.most_common(n),\n",
    "                           columns=['word','count']).sort_values('count',\n",
    "                                                                 ascending=True)\n",
    "    most_common.set_index('word').tail(n).plot(kind='barh',figsize=figsize)\n",
    "    \n",
    "plot_most_common(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with URLs, RT's, and @'s with `TweetTokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:16.001481Z",
     "start_time": "2021-06-03T15:28:13.956447Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "tokenizer = TweetTokenizer(preserve_case=False,)\n",
    "tweet_tokens = tokenizer.tokenize(','.join(corpus))\n",
    "tweet_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:16.509881Z",
     "start_time": "2021-06-03T15:28:16.003188Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a new freq dist for tweet tokens and plot most common\n",
    "tweet_freq = FreqDist(tweet_tokens)\n",
    "plot_most_common(tweet_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Better...but what's our next issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:16.514294Z",
     "start_time": "2021-06-03T15:28:16.511912Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a list of stopwords to remove\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:16.522733Z",
     "start_time": "2021-06-03T15:28:16.516877Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all the stop words in the English language and preview first 10\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:16.529016Z",
     "start_time": "2021-06-03T15:28:16.524640Z"
    }
   },
   "outputs": [],
   "source": [
    "## Add punctuation to stopwords_list\n",
    "stopwords_list.extend(string.punctuation)\n",
    "stopwords_list[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:16.533054Z",
     "start_time": "2021-06-03T15:28:16.530718Z"
    }
   },
   "outputs": [],
   "source": [
    "## Add the additional Tweet Punctuation below to stopwords_list\n",
    "additional_punc = ['“','”','...',\"''\",'’','``']\n",
    "stopwords_list.extend(additional_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:16.537833Z",
     "start_time": "2021-06-03T15:28:16.534808Z"
    }
   },
   "outputs": [],
   "source": [
    "## Commentary on not always accepting what is or isn't in stopwords\n",
    "print('until' in stopwords_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:16.542511Z",
     "start_time": "2021-06-03T15:28:16.539678Z"
    }
   },
   "outputs": [],
   "source": [
    "## Remove until from stopwords_list and check for it again\n",
    "stopwords_list.remove('until')\n",
    "print('until' in stopwords_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:17.457389Z",
     "start_time": "2021-06-03T15:28:16.544438Z"
    }
   },
   "outputs": [],
   "source": [
    "## Remove stopwords\n",
    "stopped_tokens= [w.lower() for w in tweet_tokens  if w.lower() not in stopwords_list]\n",
    "stopped_tokens[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:17.851767Z",
     "start_time": "2021-06-03T15:28:17.458993Z"
    }
   },
   "outputs": [],
   "source": [
    "## Remake the FreqDist from stopped_tokens\n",
    "freq = FreqDist(stopped_tokens)\n",
    "plot_most_common(freq,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:17.860784Z",
     "start_time": "2021-06-03T15:28:17.853593Z"
    }
   },
   "outputs": [],
   "source": [
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:17.866145Z",
     "start_time": "2021-06-03T15:28:17.862550Z"
    }
   },
   "outputs": [],
   "source": [
    "#### Appending our stopwords list\n",
    "stopwords_list.extend(['…','rt','http','https','co'])\n",
    "stopwords_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:19.241006Z",
     "start_time": "2021-06-03T15:28:17.872687Z"
    }
   },
   "outputs": [],
   "source": [
    "## Remake the FreqDist from stopped_tokens\n",
    "stopped_tokens= [w.lower() for w in tweet_tokens  if w.lower() not in stopwords_list]\n",
    "freq = FreqDist(stopped_tokens)\n",
    "# freq.most_common(100)\n",
    "plot_most_common(freq,25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Ways to Show Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Word Clouds](https://www.geeksforgeeks.org/generating-word-cloud-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:20.047300Z",
     "start_time": "2021-06-03T15:28:19.245343Z"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "## Initalize a WordCloud with our stopwords_list and no bigrams\n",
    "wordcloud = WordCloud(stopwords=stopwords_list,collocations=False)\n",
    "\n",
    "## Generate wordcloud from stopped_tokens\n",
    "wordcloud.generate(','.join(stopped_tokens))\n",
    "\n",
    "## Plot with matplotlib\n",
    "plt.figure(figsize = (12, 12), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Phases of Proprocessing/Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:20.072415Z",
     "start_time": "2021-06-03T15:28:20.049150Z"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact\n",
    "def tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "    \n",
    "    print(f\"- Tweet #{i}:\\n\")\n",
    "    print(corpus[i],'\\n')\n",
    "    tokens = word_tokenize(corpus[i])\n",
    "\n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    stopwords_list += additional_punc\n",
    "    stopped_tokens = [w.lower() for w in tokens if w.lower() not in stopwords_list]\n",
    "    \n",
    "    print(tokens,end='\\n\\n')\n",
    "    print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What recognizable pattern of characters is high on the frequency list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Bag of Words Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:21.096667Z",
     "start_time": "2021-06-03T15:28:20.074334Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "tweet_finder = nltk.BigramCollocationFinder.from_words(stopped_tokens)\n",
    "tweets_scored = tweet_finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:21.177044Z",
     "start_time": "2021-06-03T15:28:21.098944Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a DataFrame from the Bigrams\n",
    "pd.DataFrame(tweets_scored, columns=[\"Word\",\"Freq\"]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutual Information Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:21.675332Z",
     "start_time": "2021-06-03T15:28:21.178972Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "tweet_pmi_finder = nltk.BigramCollocationFinder.from_words(stopped_tokens)\n",
    "tweet_pmi_finder.apply_freq_filter(3)\n",
    "\n",
    "tweet_pmi_scored = tweet_pmi_finder.score_ngrams(bigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:21.692455Z",
     "start_time": "2021-06-03T15:28:21.677148Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a DataFrame from the Bigrams with PMI\n",
    "pd.DataFrame(tweet_pmi_scored,columns=['Words','PMI']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions [ Skippable]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regular expressions can help us capture/remove complicated patterns in our text.\n",
    "- Best regexp resource and tester: https://regex101.com/\n",
    "\n",
    "    - Make sure to check \"Python\" under Flavor menu on left side.\n",
    "    \n",
    "    \n",
    "- Let's use regular expressions to remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:21.697456Z",
     "start_time": "2021-06-03T15:28:21.694098Z"
    }
   },
   "outputs": [],
   "source": [
    "## Select an example tweet\n",
    "text =  corpus[6615]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:21.702822Z",
     "start_time": "2021-06-03T15:28:21.698883Z"
    }
   },
   "outputs": [],
   "source": [
    "## Select a second example tweet\n",
    "text2=corpus[7347]\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:21.708981Z",
     "start_time": "2021-06-03T15:28:21.704767Z"
    }
   },
   "outputs": [],
   "source": [
    "## From tjhe lessons\n",
    "from nltk import regexp_tokenize\n",
    "pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "regexp_tokenize(text,pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use regex to find/remove URLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- www.regex101.com\n",
    "    - Copy and paste example text to search\n",
    "    - Test out regular expressions and see what they pick up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:21.713752Z",
     "start_time": "2021-06-03T15:28:21.710740Z"
    }
   },
   "outputs": [],
   "source": [
    "print(text,text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:21.718873Z",
     "start_time": "2021-06-03T15:28:21.715364Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "re.findall(r\"(https://\\w*\\.\\w*/+\\w+)\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:21.725283Z",
     "start_time": "2021-06-03T15:28:21.721153Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text,regex=True):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "\n",
    "    ## tokenize text\n",
    "    if regex:\n",
    "        pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "        tokens= regexp_tokenize(text,pattern)\n",
    "    else:\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    \n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    \n",
    "    return stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:21.730287Z",
     "start_time": "2021-06-03T15:28:21.727091Z"
    }
   },
   "outputs": [],
   "source": [
    "## Other uses of RegEx for Tweet preprocessing\n",
    "import re\n",
    "\n",
    "def find_urls(string): \n",
    "    return re.findall(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\",string)\n",
    "\n",
    "def find_hashtags(string):\n",
    "    return re.findall(r'\\#\\w*',string)\n",
    "\n",
    "def find_retweets(string):\n",
    "    return re.findall(r'RT [@]?\\w*:',string)\n",
    "\n",
    "def find_mentions(string):\n",
    "    return re.findall(r'\\@\\w*',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:21.734961Z",
     "start_time": "2021-06-03T15:28:21.731914Z"
    }
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:21.740577Z",
     "start_time": "2021-06-03T15:28:21.737123Z"
    }
   },
   "outputs": [],
   "source": [
    "find_urls(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:21.745167Z",
     "start_time": "2021-06-03T15:28:21.741992Z"
    }
   },
   "outputs": [],
   "source": [
    "find_mentions(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:23.298332Z",
     "start_time": "2021-06-03T15:28:21.747359Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('feet')) # foot\n",
    "print(lemmatizer.lemmatize('running')) # run [?!] Does not match expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:23.302810Z",
     "start_time": "2021-06-03T15:28:23.299966Z"
    }
   },
   "outputs": [],
   "source": [
    "text_in =  corpus[6615]\n",
    "text_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:23.308393Z",
     "start_time": "2021-06-03T15:28:23.304423Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_tweet(text,as_lemmas=False,as_tokens=True):\n",
    "#     text=text.copy()\n",
    "    for x in find_urls(text):\n",
    "        text = text.replace(x,'')\n",
    "        \n",
    "    for x in find_retweets(text):\n",
    "        text = text.replace(x,'')    \n",
    "        \n",
    "    for x in find_hashtags(text):\n",
    "        text = text.replace(x,'')    \n",
    "\n",
    "    if as_lemmas:\n",
    "        from nltk.stem.wordnet import WordNetLemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = lemmatizer.lemmatize(text)\n",
    "    \n",
    "    if as_tokens:\n",
    "        text = clean_text(text)\n",
    "    \n",
    "    if len(text)==0:\n",
    "        text=''\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:23.331833Z",
     "start_time": "2021-06-03T15:28:23.310186Z"
    }
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_processed_text(i=(0,len(corpus)-1)):\n",
    "    text_in = corpus[i]#.copy()\n",
    "    print(text_in)\n",
    "    text_out = process_tweet(text_in)\n",
    "    print(text_out)\n",
    "    text_out2 = process_tweet(text_in,as_lemmas=True)\n",
    "    print(text_out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:23.337093Z",
     "start_time": "2021-06-03T15:28:23.333552Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For computers to process text it needs to be converted to a numerical representation of the text.\n",
    "- **There are several different ways we can vectorize our text:**\n",
    "    - Count vectorization\n",
    "    - Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "        -  Used for multiple texts\n",
    "    - Word Embeddings (Deep NLP)\n",
    "    \n",
    "    \n",
    ">- **_Term Frequency_** is calculated with the following formula:\n",
    "$$ \\text{Term Frequency}(t) = \\frac{\\text{number of times it appears in a document}} {\\text{total number of terms in the document}} $$ <br>\n",
    "- Which can also be represented as:\n",
    "$$\\begin{align}\n",
    " \\text{tf}_{i,j} = \\dfrac{n_{i,j}}{\\displaystyle \\sum_k n_{i,j} }\n",
    "\\end{align} $$\n",
    "\n",
    "> - **_Inverse Document Frequency_** is calculated with the following formula:\n",
    "$$ IDF(t) = log_e(\\frac{\\text{Total Number of Documents}}{\\text{Number of Documents with it in it}})$$<br>\n",
    "- Which can also be represented as: \n",
    "$$\\begin{align}\n",
    "idf(w) = \\log \\dfrac{N}{df_t}\n",
    "\\end{align} $$\n",
    "\n",
    "> The **_TF-IDF_** value for a given word in a given document is just found by multiplying the two!\n",
    "$$ \\begin{align}\n",
    "w_{i,j} = tf_{i,j} \\times \\log \\dfrac{N}{df_i} \\\\\n",
    "tf_{i,j} = \\text{number of occurences of } i \\text{ in} j \\\\\n",
    "df_i = \\text{number of documents containing } i \\\\\n",
    "N = \\text{total number of documents}\n",
    "\\end{align} $$\n",
    "\n",
    "- There are additional ways to vectorize using Deep Neural Networks to create Word Embeddings (see Module 4 > Appendix: Deep NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Feature Engineering for Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Do we remove stop words or not?    \n",
    "* Do we stem or lemmatize our text data, or leave the words as is?   \n",
    "* Is basic tokenization enough, or do we need to support special edge cases through the use of regex?  \n",
    "* Do we use the entire vocabulary, or just limit the model to a subset of the most frequently used words? If so, how many?  \n",
    "* Do we engineer other features, such as bigrams, or POS tags, or Mutual Information Scores?   \n",
    "* What sort of vectorization should we use in our model? Boolean Vectorization? Count Vectorization? TF-IDF? More advanced vectorization strategies such as Word2Vec?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ACTIVITY: Text Classification - Finding Trump**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Task - Finding Trump\n",
    "\n",
    "> - All presidents have staffers help maintain their social media presence on their behalf. \n",
    "- Early During His Presidency, Donald Trump refused to stop using his insecure and unofficial Android Phone\n",
    "- During this time period, his staffers were the ones Tweeting from the official presidential iPhone.\n",
    "\n",
    "> - Therefore, if we isolate our dataset to ONLY the times where Trump's account was posting from BOTH android and iphone, we can then assume that Android Tweets are Trump and that iPhone tweets are his staffers.\n",
    "\n",
    "> #### Now that we know that... let's build a NLP classification model to Find Trump!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T16:00:52.311461Z",
     "start_time": "2021-06-03T16:00:52.237925Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer #TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T16:00:53.705725Z",
     "start_time": "2021-06-03T16:00:52.658752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-12-01 14:37:57</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>My thoughts and prayers are with those affecte...</td>\n",
       "      <td>12077</td>\n",
       "      <td>65724</td>\n",
       "      <td>False</td>\n",
       "      <td>804333718999539712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01 14:38:09</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Getting ready to leave for the Great State of ...</td>\n",
       "      <td>9834</td>\n",
       "      <td>57249</td>\n",
       "      <td>False</td>\n",
       "      <td>804333771021570048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01 22:52:10</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Heading to U.S. Bank Arena in Cincinnati Ohio ...</td>\n",
       "      <td>5564</td>\n",
       "      <td>31256</td>\n",
       "      <td>False</td>\n",
       "      <td>804458095569158144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-02 02:45:18</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Thank you Ohio! Together we made history – and...</td>\n",
       "      <td>17283</td>\n",
       "      <td>72196</td>\n",
       "      <td>False</td>\n",
       "      <td>804516764562374656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-03 00:44:20</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>The President of Taiwan CALLED ME today to wis...</td>\n",
       "      <td>24700</td>\n",
       "      <td>111106</td>\n",
       "      <td>False</td>\n",
       "      <td>804848711599882240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  source  \\\n",
       "created_at                                 \n",
       "2016-12-01 14:37:57   Twitter for iPhone   \n",
       "2016-12-01 14:38:09  Twitter for Android   \n",
       "2016-12-01 22:52:10   Twitter for iPhone   \n",
       "2016-12-02 02:45:18   Twitter for iPhone   \n",
       "2016-12-03 00:44:20  Twitter for Android   \n",
       "\n",
       "                                                                  text  \\\n",
       "created_at                                                               \n",
       "2016-12-01 14:37:57  My thoughts and prayers are with those affecte...   \n",
       "2016-12-01 14:38:09  Getting ready to leave for the Great State of ...   \n",
       "2016-12-01 22:52:10  Heading to U.S. Bank Arena in Cincinnati Ohio ...   \n",
       "2016-12-02 02:45:18  Thank you Ohio! Together we made history – and...   \n",
       "2016-12-03 00:44:20  The President of Taiwan CALLED ME today to wis...   \n",
       "\n",
       "                     retweet_count  favorite_count is_retweet  \\\n",
       "created_at                                                      \n",
       "2016-12-01 14:37:57          12077           65724      False   \n",
       "2016-12-01 14:38:09           9834           57249      False   \n",
       "2016-12-01 22:52:10           5564           31256      False   \n",
       "2016-12-02 02:45:18          17283           72196      False   \n",
       "2016-12-03 00:44:20          24700          111106      False   \n",
       "\n",
       "                                 id_str  \n",
       "created_at                               \n",
       "2016-12-01 14:37:57  804333718999539712  \n",
       "2016-12-01 14:38:09  804333771021570048  \n",
       "2016-12-01 22:52:10  804458095569158144  \n",
       "2016-12-02 02:45:18  804516764562374656  \n",
       "2016-12-03 00:44:20  804848711599882240  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "## Load in the df with created_at as dt index\n",
    "finding_trump = 'finding-trump.csv'\n",
    "\n",
    "df = pd.read_csv(finding_trump, index_col='created_at', \n",
    "                 parse_dates=['created_at'])\n",
    "df.sort_index(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T16:00:53.856588Z",
     "start_time": "2021-06-03T16:00:53.849273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Twitter for iPhone      13277\n",
       "Twitter for Android       364\n",
       "Media Studio              153\n",
       "Twitter Media Studio      136\n",
       "Twitter Web Client         61\n",
       "Twitter for iPad           38\n",
       "Twitter Ads                33\n",
       "Twitter Web App             4\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check Value Counts for Source\n",
    "df['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T16:00:54.403718Z",
     "start_time": "2021-06-03T16:00:54.399427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2016-12-01 14:38:09', '2016-12-03 00:44:20',\n",
       "               '2016-12-03 01:41:30', '2016-12-03 03:06:41',\n",
       "               '2016-12-03 16:37:27', '2016-12-04 05:13:58',\n",
       "               '2016-12-04 11:41:47', '2016-12-04 11:49:06',\n",
       "               '2016-12-04 11:57:41', '2016-12-04 12:05:35',\n",
       "               ...\n",
       "               '2017-03-05 11:40:20', '2017-03-07 12:04:13',\n",
       "               '2017-03-07 12:13:59', '2017-03-07 13:13:20',\n",
       "               '2017-03-07 13:41:58', '2017-03-07 13:46:28',\n",
       "               '2017-03-07 14:14:03', '2017-03-08 12:11:25',\n",
       "               '2017-03-25 14:37:52', '2017-03-25 14:41:14'],\n",
       "              dtype='datetime64[ns]', name='created_at', length=364, freq=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get time period where Trump still had his personal Android\n",
    "index = df[ df['source']=='Twitter for Android'].index\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T16:00:54.892576Z",
     "start_time": "2021-06-03T16:00:54.889035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2016-12-01 14:38:09'), Timestamp('2017-03-25 14:41:14'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get Start_ts and end_ts\n",
    "start_ts,end_ts = index[0],index[-1]\n",
    "start_ts,end_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T16:00:55.672811Z",
     "start_time": "2021-06-03T16:00:55.660130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-12-01 14:38:09</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Getting ready to leave for the Great State of ...</td>\n",
       "      <td>9834</td>\n",
       "      <td>57249</td>\n",
       "      <td>False</td>\n",
       "      <td>804333771021570048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01 22:52:10</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Heading to U.S. Bank Arena in Cincinnati Ohio ...</td>\n",
       "      <td>5564</td>\n",
       "      <td>31256</td>\n",
       "      <td>False</td>\n",
       "      <td>804458095569158144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-02 02:45:18</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Thank you Ohio! Together we made history – and...</td>\n",
       "      <td>17283</td>\n",
       "      <td>72196</td>\n",
       "      <td>False</td>\n",
       "      <td>804516764562374656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-03 00:44:20</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>The President of Taiwan CALLED ME today to wis...</td>\n",
       "      <td>24700</td>\n",
       "      <td>111106</td>\n",
       "      <td>False</td>\n",
       "      <td>804848711599882240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-03 01:41:30</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Interesting how the U.S. sells Taiwan billions...</td>\n",
       "      <td>38805</td>\n",
       "      <td>122905</td>\n",
       "      <td>False</td>\n",
       "      <td>804863098138005504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-24 17:03:46</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Today I was pleased to announce the official a...</td>\n",
       "      <td>12933</td>\n",
       "      <td>66692</td>\n",
       "      <td>False</td>\n",
       "      <td>845320243614547968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-24 17:59:42</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Today I was thrilled to announce a commitment ...</td>\n",
       "      <td>20212</td>\n",
       "      <td>89339</td>\n",
       "      <td>False</td>\n",
       "      <td>845334323045765121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-25 13:29:17</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Happy #MedalOfHonorDay to our heroes! ➡️https:...</td>\n",
       "      <td>14139</td>\n",
       "      <td>68302</td>\n",
       "      <td>False</td>\n",
       "      <td>845628655493677056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-25 14:37:52</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>ObamaCare will explode and we will all get tog...</td>\n",
       "      <td>22518</td>\n",
       "      <td>104321</td>\n",
       "      <td>False</td>\n",
       "      <td>845645916732358656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-25 14:41:14</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Watch @JudgeJeanine on @FoxNews tonight at 9:0...</td>\n",
       "      <td>10116</td>\n",
       "      <td>51247</td>\n",
       "      <td>False</td>\n",
       "      <td>845646761704243200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>617 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  source  \\\n",
       "created_at                                 \n",
       "2016-12-01 14:38:09  Twitter for Android   \n",
       "2016-12-01 22:52:10   Twitter for iPhone   \n",
       "2016-12-02 02:45:18   Twitter for iPhone   \n",
       "2016-12-03 00:44:20  Twitter for Android   \n",
       "2016-12-03 01:41:30  Twitter for Android   \n",
       "...                                  ...   \n",
       "2017-03-24 17:03:46   Twitter for iPhone   \n",
       "2017-03-24 17:59:42   Twitter for iPhone   \n",
       "2017-03-25 13:29:17   Twitter for iPhone   \n",
       "2017-03-25 14:37:52  Twitter for Android   \n",
       "2017-03-25 14:41:14  Twitter for Android   \n",
       "\n",
       "                                                                  text  \\\n",
       "created_at                                                               \n",
       "2016-12-01 14:38:09  Getting ready to leave for the Great State of ...   \n",
       "2016-12-01 22:52:10  Heading to U.S. Bank Arena in Cincinnati Ohio ...   \n",
       "2016-12-02 02:45:18  Thank you Ohio! Together we made history – and...   \n",
       "2016-12-03 00:44:20  The President of Taiwan CALLED ME today to wis...   \n",
       "2016-12-03 01:41:30  Interesting how the U.S. sells Taiwan billions...   \n",
       "...                                                                ...   \n",
       "2017-03-24 17:03:46  Today I was pleased to announce the official a...   \n",
       "2017-03-24 17:59:42  Today I was thrilled to announce a commitment ...   \n",
       "2017-03-25 13:29:17  Happy #MedalOfHonorDay to our heroes! ➡️https:...   \n",
       "2017-03-25 14:37:52  ObamaCare will explode and we will all get tog...   \n",
       "2017-03-25 14:41:14  Watch @JudgeJeanine on @FoxNews tonight at 9:0...   \n",
       "\n",
       "                     retweet_count  favorite_count is_retweet  \\\n",
       "created_at                                                      \n",
       "2016-12-01 14:38:09           9834           57249      False   \n",
       "2016-12-01 22:52:10           5564           31256      False   \n",
       "2016-12-02 02:45:18          17283           72196      False   \n",
       "2016-12-03 00:44:20          24700          111106      False   \n",
       "2016-12-03 01:41:30          38805          122905      False   \n",
       "...                            ...             ...        ...   \n",
       "2017-03-24 17:03:46          12933           66692      False   \n",
       "2017-03-24 17:59:42          20212           89339      False   \n",
       "2017-03-25 13:29:17          14139           68302      False   \n",
       "2017-03-25 14:37:52          22518          104321      False   \n",
       "2017-03-25 14:41:14          10116           51247      False   \n",
       "\n",
       "                                 id_str  \n",
       "created_at                               \n",
       "2016-12-01 14:38:09  804333771021570048  \n",
       "2016-12-01 22:52:10  804458095569158144  \n",
       "2016-12-02 02:45:18  804516764562374656  \n",
       "2016-12-03 00:44:20  804848711599882240  \n",
       "2016-12-03 01:41:30  804863098138005504  \n",
       "...                                 ...  \n",
       "2017-03-24 17:03:46  845320243614547968  \n",
       "2017-03-24 17:59:42  845334323045765121  \n",
       "2017-03-25 13:29:17  845628655493677056  \n",
       "2017-03-25 14:37:52  845645916732358656  \n",
       "2017-03-25 14:41:14  845646761704243200  \n",
       "\n",
       "[617 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Slice out the data from start_ts to end_ts\n",
    "df = df.loc[start_ts:end_ts]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T16:00:56.542652Z",
     "start_time": "2021-06-03T16:00:56.537331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Twitter for Android    0.589951\n",
       "Twitter for iPhone     0.387358\n",
       "Twitter Web Client     0.022690\n",
       "Name: source, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check new value counts \n",
    "df['source'].value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T16:00:57.489746Z",
     "start_time": "2021-06-03T16:00:57.478172Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-12-01 14:38:09</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Getting ready to leave for the Great State of ...</td>\n",
       "      <td>9834</td>\n",
       "      <td>57249</td>\n",
       "      <td>False</td>\n",
       "      <td>804333771021570048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01 22:52:10</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Heading to U.S. Bank Arena in Cincinnati Ohio ...</td>\n",
       "      <td>5564</td>\n",
       "      <td>31256</td>\n",
       "      <td>False</td>\n",
       "      <td>804458095569158144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-02 02:45:18</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Thank you Ohio! Together we made history – and...</td>\n",
       "      <td>17283</td>\n",
       "      <td>72196</td>\n",
       "      <td>False</td>\n",
       "      <td>804516764562374656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-03 00:44:20</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>The President of Taiwan CALLED ME today to wis...</td>\n",
       "      <td>24700</td>\n",
       "      <td>111106</td>\n",
       "      <td>False</td>\n",
       "      <td>804848711599882240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-03 01:41:30</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Interesting how the U.S. sells Taiwan billions...</td>\n",
       "      <td>38805</td>\n",
       "      <td>122905</td>\n",
       "      <td>False</td>\n",
       "      <td>804863098138005504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-24 17:03:46</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Today I was pleased to announce the official a...</td>\n",
       "      <td>12933</td>\n",
       "      <td>66692</td>\n",
       "      <td>False</td>\n",
       "      <td>845320243614547968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-24 17:59:42</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Today I was thrilled to announce a commitment ...</td>\n",
       "      <td>20212</td>\n",
       "      <td>89339</td>\n",
       "      <td>False</td>\n",
       "      <td>845334323045765121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-25 13:29:17</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Happy #MedalOfHonorDay to our heroes! ➡️https:...</td>\n",
       "      <td>14139</td>\n",
       "      <td>68302</td>\n",
       "      <td>False</td>\n",
       "      <td>845628655493677056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-25 14:37:52</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>ObamaCare will explode and we will all get tog...</td>\n",
       "      <td>22518</td>\n",
       "      <td>104321</td>\n",
       "      <td>False</td>\n",
       "      <td>845645916732358656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-25 14:41:14</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Watch @JudgeJeanine on @FoxNews tonight at 9:0...</td>\n",
       "      <td>10116</td>\n",
       "      <td>51247</td>\n",
       "      <td>False</td>\n",
       "      <td>845646761704243200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>603 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  source  \\\n",
       "created_at                                 \n",
       "2016-12-01 14:38:09  Twitter for Android   \n",
       "2016-12-01 22:52:10   Twitter for iPhone   \n",
       "2016-12-02 02:45:18   Twitter for iPhone   \n",
       "2016-12-03 00:44:20  Twitter for Android   \n",
       "2016-12-03 01:41:30  Twitter for Android   \n",
       "...                                  ...   \n",
       "2017-03-24 17:03:46   Twitter for iPhone   \n",
       "2017-03-24 17:59:42   Twitter for iPhone   \n",
       "2017-03-25 13:29:17   Twitter for iPhone   \n",
       "2017-03-25 14:37:52  Twitter for Android   \n",
       "2017-03-25 14:41:14  Twitter for Android   \n",
       "\n",
       "                                                                  text  \\\n",
       "created_at                                                               \n",
       "2016-12-01 14:38:09  Getting ready to leave for the Great State of ...   \n",
       "2016-12-01 22:52:10  Heading to U.S. Bank Arena in Cincinnati Ohio ...   \n",
       "2016-12-02 02:45:18  Thank you Ohio! Together we made history – and...   \n",
       "2016-12-03 00:44:20  The President of Taiwan CALLED ME today to wis...   \n",
       "2016-12-03 01:41:30  Interesting how the U.S. sells Taiwan billions...   \n",
       "...                                                                ...   \n",
       "2017-03-24 17:03:46  Today I was pleased to announce the official a...   \n",
       "2017-03-24 17:59:42  Today I was thrilled to announce a commitment ...   \n",
       "2017-03-25 13:29:17  Happy #MedalOfHonorDay to our heroes! ➡️https:...   \n",
       "2017-03-25 14:37:52  ObamaCare will explode and we will all get tog...   \n",
       "2017-03-25 14:41:14  Watch @JudgeJeanine on @FoxNews tonight at 9:0...   \n",
       "\n",
       "                     retweet_count  favorite_count is_retweet  \\\n",
       "created_at                                                      \n",
       "2016-12-01 14:38:09           9834           57249      False   \n",
       "2016-12-01 22:52:10           5564           31256      False   \n",
       "2016-12-02 02:45:18          17283           72196      False   \n",
       "2016-12-03 00:44:20          24700          111106      False   \n",
       "2016-12-03 01:41:30          38805          122905      False   \n",
       "...                            ...             ...        ...   \n",
       "2017-03-24 17:03:46          12933           66692      False   \n",
       "2017-03-24 17:59:42          20212           89339      False   \n",
       "2017-03-25 13:29:17          14139           68302      False   \n",
       "2017-03-25 14:37:52          22518          104321      False   \n",
       "2017-03-25 14:41:14          10116           51247      False   \n",
       "\n",
       "                                 id_str  \n",
       "created_at                               \n",
       "2016-12-01 14:38:09  804333771021570048  \n",
       "2016-12-01 22:52:10  804458095569158144  \n",
       "2016-12-02 02:45:18  804516764562374656  \n",
       "2016-12-03 00:44:20  804848711599882240  \n",
       "2016-12-03 01:41:30  804863098138005504  \n",
       "...                                 ...  \n",
       "2017-03-24 17:03:46  845320243614547968  \n",
       "2017-03-24 17:59:42  845334323045765121  \n",
       "2017-03-25 13:29:17  845628655493677056  \n",
       "2017-03-25 14:37:52  845645916732358656  \n",
       "2017-03-25 14:41:14  845646761704243200  \n",
       "\n",
       "[603 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Remove the Web tweets\n",
    "df = df[df['source'] != 'Twitter Web Client']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T16:00:59.389992Z",
     "start_time": "2021-06-03T16:00:59.383798Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-b41e70b2c595>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['trump_tweet'] = (df['source'] == 'Twitter for Android').astype(int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    0.603648\n",
       "0    0.396352\n",
       "Name: trump_tweet, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make new Trump Tweet Column of 0 and 1s\n",
    "df['trump_tweet'] = (df['source'] == 'Twitter for Android').astype(int)\n",
    "df['trump_tweet'].value_counts(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T16:01:02.358304Z",
     "start_time": "2021-06-03T16:01:02.353152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at\n",
       "2016-12-01 14:38:09    Getting ready to leave for the Great State of ...\n",
       "2016-12-01 22:52:10    Heading to U.S. Bank Arena in Cincinnati Ohio ...\n",
       "2016-12-02 02:45:18    Thank you Ohio! Together we made history – and...\n",
       "2016-12-03 00:44:20    The President of Taiwan CALLED ME today to wis...\n",
       "2016-12-03 01:41:30    Interesting how the U.S. sells Taiwan billions...\n",
       "                                             ...                        \n",
       "2017-03-24 17:03:46    Today I was pleased to announce the official a...\n",
       "2017-03-24 17:59:42    Today I was thrilled to announce a commitment ...\n",
       "2017-03-25 13:29:17    Happy #MedalOfHonorDay to our heroes! ➡️https:...\n",
       "2017-03-25 14:37:52    ObamaCare will explode and we will all get tog...\n",
       "2017-03-25 14:41:14    Watch @JudgeJeanine on @FoxNews tonight at 9:0...\n",
       "Name: text, Length: 603, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make X and y\n",
    "y = df['trump_tweet'].copy()\n",
    "X = df['text'].copy()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T16:01:03.174914Z",
     "start_time": "2021-06-03T16:01:03.169054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at\n",
       "2017-01-20 17:54:36    We will bring back our jobs. We will bring bac...\n",
       "2017-03-25 14:41:14    Watch @JudgeJeanine on @FoxNews tonight at 9:0...\n",
       "2017-02-25 21:53:21    I will not be attending the White House Corres...\n",
       "2017-02-15 19:17:59    Welcome to the United States @IsraeliPM Benjam...\n",
       "2017-03-15 10:55:30    Does anybody really believe that a reporter wh...\n",
       "                                             ...                        \n",
       "2016-12-15 22:27:16    Join me in Mobile Alabama on Sat. at 3pm! #Tha...\n",
       "2016-12-24 00:13:02    Vladimir Putin said today about Hillary and De...\n",
       "2017-01-20 17:53:17    January 20th 2017 will be remembered as the da...\n",
       "2017-02-15 13:13:10    The real scandal here is that classified infor...\n",
       "2016-12-23 11:58:36    my presidency. Isn't this a ridiculous shame? ...\n",
       "Name: text, Length: 422, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train Test Split (random state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,\n",
    "                                                    random_state=42)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T16:01:04.071575Z",
     "start_time": "2021-06-03T16:01:04.066879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.604265\n",
       "0    0.395735\n",
       "Name: trump_tweet, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check y_train value counts\n",
    "y_train.value_counts(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization & Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:24.650232Z",
     "start_time": "2021-06-03T15:28:24.646238Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a TweekTokenizer from nltk.tokenize (preserve_case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:24.742725Z",
     "start_time": "2021-06-03T15:28:24.651824Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a TfIdf Vectorizer using tweet tokenizer's .tokenize method\n",
    "\n",
    "\n",
    "# Vectorize data and make X_train_tfidf and X_test_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:24.747229Z",
     "start_time": "2021-06-03T15:28:24.744456Z"
    }
   },
   "outputs": [],
   "source": [
    "## Compare the shape of X_train to X_train_tfidf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:24.752124Z",
     "start_time": "2021-06-03T15:28:24.749172Z"
    }
   },
   "outputs": [],
   "source": [
    "## Check the len of the vectorizer's vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:24.923695Z",
     "start_time": "2021-06-03T15:28:24.753459Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make and fit a random forest  (class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:24.940250Z",
     "start_time": "2021-06-03T15:28:24.925625Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get predictions for train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:24.948532Z",
     "start_time": "2021-06-03T15:28:24.941932Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate_classification(model, X_test_tf,y_test,cmap='Greens',\n",
    "                            normalize='true',classes=None,figsize=(10,4),\n",
    "                            X_train = None, y_train = None,):\n",
    "    \"\"\"Evaluates a scikit-learn binary classification model.\n",
    "\n",
    "    Args:\n",
    "        model (classifier): any sklearn classification model.\n",
    "        X_test_tf (Frame or Array): X data\n",
    "        y_test (Series or Array): y data\n",
    "        cmap (str, optional): Colormap for confusion matrix. Defaults to 'Greens'.\n",
    "        normalize (str, optional): normalize argument for plot_confusion_matrix. \n",
    "                                    Defaults to 'true'.\n",
    "        classes (list, optional): List of class names for display. Defaults to None.\n",
    "        figsize (tuple, optional): figure size Defaults to (8,4).\n",
    "        \n",
    "        X_train (Frame or Array, optional): If provided, compare model.score \n",
    "                                for train and test. Defaults to None.\n",
    "        y_train (Series or Array, optional): If provided, compare model.score \n",
    "                                for train and test. Defaults to None.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Get Predictions and Classification Report\n",
    "    y_hat_test = model.predict(X_test_tf)\n",
    "    print(metrics.classification_report(y_test, y_hat_test,target_names=classes))\n",
    "    \n",
    "    ## Plot Confusion Matrid and roc curve\n",
    "    fig,ax = plt.subplots(ncols=2, figsize=figsize)\n",
    "    metrics.plot_confusion_matrix(model, X_test_tf,y_test,cmap=cmap, \n",
    "                                  normalize=normalize,display_labels=classes,\n",
    "                                 ax=ax[0])\n",
    "    \n",
    "    ## if roc curve erorrs, delete second ax\n",
    "    try:\n",
    "        curve = metrics.plot_roc_curve(model,X_test_tf,y_test,ax=ax[1])\n",
    "        curve.ax_.grid()\n",
    "        curve.ax_.plot([0,1],[0,1],ls=':')\n",
    "        fig.tight_layout()\n",
    "    except:\n",
    "        fig.delaxes(ax[1])\n",
    "    plt.show()\n",
    "    \n",
    "    ## Add comparing Scores if X_train and y_train provided.\n",
    "    if (X_train is not None) & (y_train is not None):\n",
    "        print(f\"Training Score = {model.score(X_train,y_train):.2f}\")\n",
    "        print(f\"Test Score = {model.score(X_test_tf,y_test):.2f}\")\n",
    "        \n",
    "    \n",
    "def plot_importance(tree, X_train_df, top_n=20,figsize=(10,10)):\n",
    "    \n",
    "    df_importance = pd.Series(tree.feature_importances_,\n",
    "                              index=X_train_df.columns)\n",
    "    df_importance.sort_values(ascending=True).tail(top_n).plot(\n",
    "        kind='barh',figsize=figsize,title='Feature Importances',\n",
    "    ylabel='Feature',)\n",
    "    return df_importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:25.299723Z",
     "start_time": "2021-06-03T15:28:24.950190Z"
    }
   },
   "outputs": [],
   "source": [
    "## Evaluate Model using function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:25.544859Z",
     "start_time": "2021-06-03T15:28:25.302182Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the top 30 most important features\n",
    "with plt.style.context('seaborn-talk'):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines and GridSearch for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You may want to to this process in multiple steps (first Count Vectorize, then transform to TF or TF-IDF.\n",
    "- Can then use these in a Pipeline to be able to GridSearch more aspects of the text preprocessing\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer #TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_pipe = Pipeline(steps=[\n",
    "    ('count_vectorizer',CountVectorizer()),\n",
    "    ('tf_transformer',TfidfTransformer(use_idf=False))\n",
    "])\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:25.549489Z",
     "start_time": "2021-06-03T15:28:25.547015Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:25.562538Z",
     "start_time": "2021-06-03T15:28:25.551966Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a text preprocessing pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:25.579632Z",
     "start_time": "2021-06-03T15:28:25.574894Z"
    }
   },
   "outputs": [],
   "source": [
    "## Remind ourselves what X_train looks like \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:25.640144Z",
     "start_time": "2021-06-03T15:28:25.584582Z"
    }
   },
   "outputs": [],
   "source": [
    "## Test out the text pipeline on X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:25.686248Z",
     "start_time": "2021-06-03T15:28:25.641736Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a full pipeline with the random forest model as the second step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:26.390485Z",
     "start_time": "2021-06-03T15:28:25.687813Z"
    }
   },
   "outputs": [],
   "source": [
    "## Modeling with full pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearching NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:28:26.394658Z",
     "start_time": "2021-06-03T15:28:26.392462Z"
    }
   },
   "outputs": [],
   "source": [
    "set_config(display='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:53.202961Z",
     "start_time": "2021-06-03T15:28:26.396495Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a tokenizer with TweetTokenizer\n",
    "\n",
    "## Make params grid\n",
    "## transformer params: use_idf/norm,smooth_idf\n",
    "### tokenizers: try different TweetTokeinzier\n",
    "\n",
    "## Vectorizer:stop_wqords/max_df/min_df,\n",
    "\n",
    "## randomforest: criterin/max_depth\n",
    "params = {}\n",
    "\n",
    "## Make and fit grid\n",
    "\n",
    "\n",
    "## Display best params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:53.294951Z",
     "start_time": "2021-06-03T15:31:53.205786Z"
    }
   },
   "outputs": [],
   "source": [
    "## Evluate the best_estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:53.303598Z",
     "start_time": "2021-06-03T15:31:53.297468Z"
    }
   },
   "outputs": [],
   "source": [
    "## Check the named steps for text_pipe, count_vectorizer, and get params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:53.326988Z",
     "start_time": "2021-06-03T15:31:53.313296Z"
    }
   },
   "outputs": [],
   "source": [
    "## Which topkenizer did it use?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:54.150501Z",
     "start_time": "2021-06-03T15:31:53.329305Z"
    }
   },
   "outputs": [],
   "source": [
    "### evalaute the best pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get feature importances as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:54.261749Z",
     "start_time": "2021-06-03T15:31:54.153507Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make x-train and x-test from text_pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:54.269495Z",
     "start_time": "2021-06-03T15:31:54.264089Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:54.278177Z",
     "start_time": "2021-06-03T15:31:54.271974Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get CountVectorizer from pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:54.293713Z",
     "start_time": "2021-06-03T15:31:54.288128Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get feature naames from vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:54.300190Z",
     "start_time": "2021-06-03T15:31:54.296024Z"
    }
   },
   "outputs": [],
   "source": [
    "## Check the len of features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:54.324687Z",
     "start_time": "2021-06-03T15:31:54.303706Z"
    }
   },
   "outputs": [],
   "source": [
    "## Check the length of feature importanceas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:54.352985Z",
     "start_time": "2021-06-03T15:31:54.327193Z"
    }
   },
   "outputs": [],
   "source": [
    "## Save feature importances as a series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:54.808384Z",
     "start_time": "2021-06-03T15:31:54.355603Z"
    }
   },
   "outputs": [],
   "source": [
    "## Plot feature importances\n",
    "with plt.style.context('seaborn-talk'):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:54.872259Z",
     "start_time": "2021-06-03T15:31:54.816031Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Saving the probability of most important words for each class\n",
    "top_word_probs = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:54.969267Z",
     "start_time": "2021-06-03T15:31:54.874730Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make into a df, transpose, and display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`Part 2` Topic 28: Bayesian Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Understand how Bayes theorem can be applied to classify data using conditional probabilities.\n",
    "- Revisit our Topic 17 study group example of Maximum Likelihood Estimation.\n",
    "\n",
    "- Understand Gaussian Naive Bayes and how it uses the Probability Density Function of a Normal Distribution \n",
    "\n",
    "<!-- - Understand the \"underflow\" issue and how to fix. -->\n",
    "\n",
    "- Apply naive bayes for tabular data and text data\n",
    "-  Document Classification with Naive Bayes - Finding Trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- $$ \\large P(A|B) = \\dfrac{P(B|A)(A)}{P(B)}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$ \\Large P(y|x_1, x_2, ..., x_n) = \\frac{P(y)\\prod_{i}^{n}P(x_i|y)}{P(x_1, x_2, ..., x_n)}$$ \n",
    " -->\n",
    "\n",
    "***The Bayesian interpretation of this formula is***\n",
    "\n",
    "\n",
    "\n",
    "$$ \\large P(A|B) = \\dfrac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "\n",
    "$$ \\large \\text{Posterior} = \\dfrac{\\text{Likelihood} \\cdot \\text{Prior}}{\\text{Evidence}}$$\n",
    "\n",
    "<img src =\"ds-nlp_modeling-main/img/naive_bayes_icon.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Revisit Topic 17 MLE Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes [Non-Text Data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gaussian Naive Bayes makes the assumption that our probabilities follow a normal distribution.\n",
    "- It uses the Probability Density Function for a Normal (Gaussian) Distribution to get point estimates of the probabilities.\n",
    "\n",
    "> **Note: above we used the normal distribution probability density function to estimate likelihood, which is essentiall what Guassian Naive Bayes does!!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:54.989001Z",
     "start_time": "2021-06-03T15:31:54.971708Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = pd.DataFrame(iris.data)\n",
    "X.columns = iris.feature_names\n",
    "\n",
    "y = pd.DataFrame(iris.target)\n",
    "y.columns = ['Target']\n",
    "\n",
    "df = pd.concat([X, y], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:54.999710Z",
     "start_time": "2021-06-03T15:31:54.991390Z"
    }
   },
   "outputs": [],
   "source": [
    "## Getting names of flowers\n",
    "target_map = dict(zip([0,1,2],iris.target_names))\n",
    "\n",
    "df['Target'] = df['Target'].map(target_map)\n",
    "df['Target'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:55.019369Z",
     "start_time": "2021-06-03T15:31:55.002632Z"
    }
   },
   "outputs": [],
   "source": [
    "## Train test split\n",
    "X_tr, X_te,y_tr,y_te = train_test_split(X,y)\n",
    "\n",
    "## Saving df-train for later\n",
    "df_train = pd.concat([X_tr,y_tr],axis=1)\n",
    "df_train['Target'] = df_train['Target'].map(target_map)\n",
    "X_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:57.410041Z",
     "start_time": "2021-06-03T15:31:55.022959Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checking Distribution of each feature by taargets\n",
    "sns.set_context('notebook')\n",
    "for col in X.columns:\n",
    "    sns.displot(data=df, x=col, kind='kde',hue='Target',aspect=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Each flower's characteristics form a normal-ish distribution with a specific mean and std. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the Normal Distribution Probability Density function, we can calculate the likelihoods of a specific flower's stats for each of our target classes. \n",
    "- Whichever likelihood is highest will determine the model's prediction. \n",
    "\n",
    "$$ \\Large P(x_i|y) = \\frac{1}{\\sqrt{2 \\pi \\sigma_i^2}}e^{\\frac{-(x-\\mu_i)^2}{2\\sigma_i^2}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:57.440706Z",
     "start_time": "2021-06-03T15:31:57.412100Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get the mean and std of each target\n",
    "aggs = df_train.groupby('Target').agg(['mean', 'std'])\n",
    "aggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:57.449115Z",
     "start_time": "2021-06-03T15:31:57.443156Z"
    }
   },
   "outputs": [],
   "source": [
    "## Selecting an example flower to examine\n",
    "ex_flower_stats = df_train.iloc[20]\n",
    "ex_flower_label = ex_flower_stats.pop('Target')\n",
    "ex_flower_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:57.456193Z",
     "start_time": "2021-06-03T15:31:57.452578Z"
    }
   },
   "outputs": [],
   "source": [
    "## get list of classes and list  of stats\n",
    "classes = df_train[\"Target\"].unique()\n",
    "classes\n",
    "\n",
    "stats_list = ex_flower_stats.index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Large P(y|x_1, x_2, ..., x_n) = \\frac{P(y)\\prod_{i}^{n}P(x_i|y)}{P(x_1, x_2, ..., x_n)}$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:57.493688Z",
     "start_time": "2021-06-03T15:31:57.466904Z"
    }
   },
   "outputs": [],
   "source": [
    "## Function from topicn 17 study group\n",
    "import math\n",
    "def calc_likelihood(x,mu,std):\n",
    "    \"\"\"Write a function to calculate the expected value at \n",
    "    a particular point using the equation above.\"\"\"\n",
    "   \n",
    "    e_num = math.e**((-1*(x-mu)**2)/(2*std**2))\n",
    "    denom = std * np.sqrt(math.pi*2 )\n",
    "    return 1/denom*e_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:57.493688Z",
     "start_time": "2021-06-03T15:31:57.466904Z"
    }
   },
   "outputs": [],
   "source": [
    "## Calculate Likelihood of belonging to each class\n",
    "likelihoods = {}\n",
    "for stat in stats_list:\n",
    "#     for class_ in classes:\n",
    "    setosa = calc_likelihood(ex_flower_stats.loc[stat],\n",
    "                                     aggs.loc['setosa',(stat,'mean')],\n",
    "                                      aggs.loc['setosa',(stat,'std')])\n",
    "    versicolor = calc_likelihood(ex_flower_stats.loc[stat],\n",
    "                                 aggs.loc['versicolor',(stat,'mean')],\n",
    "                                  aggs.loc['versicolor',(stat,'std')])\n",
    "    virginica = calc_likelihood(ex_flower_stats.loc[stat],\n",
    "                                 aggs.loc['virginica',(stat,'mean')],\n",
    "                                  aggs.loc['virginica',(stat,'std')])\n",
    "    likelihoods[stat]={'setosa':setosa,\n",
    "               'versicolor':versicolor,\n",
    "               'virginica':virginica}\n",
    "    \n",
    "df_likelihoods = pd.DataFrame(likelihoods)\n",
    "df_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:57.502733Z",
     "start_time": "2021-06-03T15:31:57.496597Z"
    }
   },
   "outputs": [],
   "source": [
    "## Which likelihhods are the largest\n",
    "df_likelihoods.idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is how Gaussian Naive Bayes makes it predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:57.797565Z",
     "start_time": "2021-06-03T15:31:57.505567Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "bayes = GaussianNB()\n",
    "bayes.fit(X_tr,y_tr)\n",
    "evaluate_classification(bayes,X_te,y_te,X_train=X_tr,y_train=y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ \\large P(\\text{Spam | Word}) = \\dfrac{P(\\text{Word | Spam})P(\\text{Spam})}{P(\\text{Word})}$$  \n",
    "\n",
    "- Where $P(\\text{Word | Spam})$ is\n",
    "\n",
    " $$ \\large P(\\text{Word | Spam}) = \\dfrac{\\text{Word Frequency in Document}}{\\text{Word Frequency Across All Spam Documents}}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"However, this formulation has a problem: **what if you encounter a word in the test set that was not present in the training set?** This new word would have a frequency of zero! To effectively counteract these issues, Laplacian smoothing is often used giving:\"  \n",
    "\n",
    "- ***Laplacian smoothing:***\n",
    "\n",
    " $$P(\\text{Word | Spam}) = \\dfrac{\\text{Word Frequency in Document} + 1}{\\text{Word Frequency Across All Spam Documents + Number of Words in Corpus Vocabulary}}$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⏰ **Activity Continued:  Finding Trump with Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:57.909661Z",
     "start_time": "2021-06-03T15:31:57.812253Z"
    }
   },
   "outputs": [],
   "source": [
    "## Use text pipe to get X_train,X_test\n",
    "X_train_pipe = text_pipe.fit_transform(X_train)\n",
    "X_test_pipe = text_pipe.transform(X_test)\n",
    "X_train_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:31:58.395664Z",
     "start_time": "2021-06-03T15:31:57.912431Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Make a naive bayes classifier\n",
    "nb_classifier = MultinomialNB()#alpha = 1.0e-08)\n",
    "nb_classifier.fit(X_train_pipe,y_train)\n",
    "evaluate_classification(nb_classifier,X_test_pipe,y_test, X_train=X_train_pipe,\n",
    "                       y_train=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:33:01.644146Z",
     "start_time": "2021-06-03T15:31:58.398235Z"
    }
   },
   "outputs": [],
   "source": [
    "### Make a new pipeline for bayes\n",
    "\n",
    "nb_pipe = Pipeline(steps=[\n",
    "    ('text_pipe',text_pipe),\n",
    "    ('clf',MultinomialNB())\n",
    "])\n",
    "\n",
    "\n",
    "params = {'text_pipe__tf_transformer__use_idf':[True, False],\n",
    "          'text_pipe__tf_transformer__norm':['l2','l1'],\n",
    "          'text_pipe__tf_transformer__use_idf':[True,False],\n",
    "          'text_pipe__tf_transformer__smooth_idf':[True,False],\n",
    "          'text_pipe__count_vectorizer__tokenizer':[ \n",
    "#               None,                                       \n",
    "              TweetTokenizer(preserve_case=True).tokenize,\n",
    "              TweetTokenizer(preserve_case=False).tokenize],\n",
    "          \n",
    "          'text_pipe__count_vectorizer__stop_words':[None,stopwords_list],\n",
    "          'text_pipe__count_vectorizer__max_df':[1.0,0.95,0.9],\n",
    "          'text_pipe__count_vectorizer__min_df':[1,2,3],\n",
    "          \n",
    "         'clf__alpha':[0, 1],\n",
    "         'clf__fit_prior':[True,False]}\n",
    "\n",
    "\n",
    "## Make and fit grid\n",
    "grid = GridSearchCV(nb_pipe,params,cv=3,scoring='recall_macro',\n",
    "                   verbose=2, n_jobs=-1)\n",
    "grid.fit(X_train,y_train)\n",
    "## Display best params\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:33:02.290863Z",
     "start_time": "2021-06-03T15:33:01.646462Z"
    }
   },
   "outputs": [],
   "source": [
    "## Evluate the best_estimator\n",
    "best_pipe = grid.best_estimator_\n",
    "evaluate_classification(best_pipe,X_test,y_test,X_train=X_train,y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:33:02.710058Z",
     "start_time": "2021-06-03T15:33:02.293425Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raise Exception(\"Stop!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## T-SNE (for Student Question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:33:02.732553Z",
     "start_time": "2021-06-03T15:28:07.476Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_pipe.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:33:02.734214Z",
     "start_time": "2021-06-03T15:28:07.481Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:33:02.735815Z",
     "start_time": "2021-06-03T15:28:07.486Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## TSNE For Visualizing High Dimensional Data\n",
    "t_sne_object_3d = TSNE(n_components=3)\n",
    "transformed_data_3d = t_sne_object_3d.fit_transform(X_train_pipe)\n",
    "transformed_data_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:33:02.737269Z",
     "start_time": "2021-06-03T15:28:07.491Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Separate into Trump/Not Trump\n",
    "trump = transformed_data_3d[y_train==1]\n",
    "not_trump = transformed_data_3d[y_train==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:33:02.738735Z",
     "start_time": "2021-06-03T15:28:07.496Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter(trump[:,0],trump[:,1],\n",
    "           trump[:,2],c='orange',label='Trump')\n",
    "ax.scatter(not_trump[:,0],not_trump[:,1],\n",
    "           not_trump[:,2],c='black',label='Not Trump')\n",
    "ax.legend()\n",
    "ax.view_init(30, 10)\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:33:02.740571Z",
     "start_time": "2021-06-03T15:28:07.501Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## TSNE For Visualizing High Dimensional Data\n",
    "t_sne_object_2d = TSNE(n_components=2)\n",
    "transformed_data_2d = t_sne_object_2d.fit_transform(X_train_pipe)\n",
    "## Separate into Trump/Not Trump\n",
    "trump = transformed_data_2d[y_train==1]\n",
    "not_trump = transformed_data_2d[y_train==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:33:02.742526Z",
     "start_time": "2021-06-03T15:28:07.506Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(20,10))\n",
    "ax.scatter(trump[:,0],trump[:,1],c='orange',label='Trump')\n",
    "ax.scatter(not_trump[:,0],not_trump[:,1],c='black',label='Not Trump')\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Interactive Tokenizer Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T15:33:02.744566Z",
     "start_time": "2021-06-03T15:28:07.512Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact\n",
    "def tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "    \n",
    "    print(f\"- Tweet #{i}:\\n\")\n",
    "    print(corpus[i],'\\n')\n",
    "    tokens = word_tokenize(corpus[i])\n",
    "\n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    \n",
    "    print(tokens,end='\\n\\n')\n",
    "    print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### NLP Vocabulary\n",
    "- Corpus\n",
    "    - Body of text\n",
    "    \n",
    "- Bag of Words\n",
    "    - Collection of all words from a corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Use https://regex101.com/ to test out regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Context-Free Grammers and POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-context-free-grammars-and-POS-tagging-online-ds-ft-100719/master/images/new_LevelsOfLanguage-Graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Syntax and Meaning Can be Difficult for Computers \n",
    "\n",
    "In English, sentences consist of a **_Noun Phrase_** followed by a **_Verb Phrase_**, which may optionally be followed by a **_Prepositional Phrase_**.\n",
    "\n",
    "This ***seems simple, but it gets more tricky*** when we realize that there is a recursive structure to these phrases.\n",
    "\n",
    "- A noun phrase may consist of multiple smaller noun phrases, and in some cases, even a verb phrase. \n",
    "- Similarly, a verb phrase can consist of multiple smaller verb phrases and noun phrases, which can themselves be made up of smaller noun phrases and verb phrases. \n",
    "\n",
    "\n",
    "This leads levels of **_ambiguity_** that can be troublesome for computers. NLTK's documentation explains this by examining the classic Groucho Marx joke:\n",
    "\n",
    "> ***\"While hunting in Africa, I shot an elephant in my pajamas. How he got into my pajamas, I don't know.\"***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-context-free-grammars-and-POS-tagging-online-ds-ft-100719/master/images/parse_tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## [MANUAL] Gaussian Naive Bayes [Non-Text Data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Gaussian Naive Bayes makes the assumption that our probabilities follow a normal distribution.\n",
    "- It uses the Probability Density Function for a Normal (Gaussian) Distribution to get point estimates of the probabilities.\n",
    "\n",
    "> **Note: above we used the normal distribution probability density function to estimate likelihood, which is essentiall what Guassian Naive Bayes does!!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-03T15:28:07.404Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = pd.DataFrame(iris.data)\n",
    "X.columns = iris.feature_names\n",
    "\n",
    "y = pd.DataFrame(iris.target)\n",
    "y.columns = ['Target']\n",
    "\n",
    "df = pd.concat([X, y], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-03T15:28:07.409Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target_map = dict(zip([0,1,2],iris.target_names))\n",
    "target_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-03T15:28:07.414Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['Target'].map(target_map).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Suppose we have three competing hypotheses $\\{versicolor, virginica, setosa\\}$ that would explain our evidence $e$.\n",
    "    - Then we could use Bayes's Theorem to calculate the posterior probabilities for each of these three:\n",
    "        - $\\large P(versicolor|e) = \\frac{P(versicolor)P(e|versicolor)}{P(e)}$\n",
    "        - $\\large P(virginica|e) = \\frac{P(virginica)P(e|virginica)}{P(e)}$\n",
    "        - $\\large P(h_3|e) = \\frac{P(h_3)P(e|h_3)}{P(e)}$\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-03T15:28:07.420Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_tr, X_te,y_tr,y_te = train_test_split(X,y)\n",
    "X_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ \\Large P(x_i|y) = \\frac{1}{\\sqrt{2 \\pi \\sigma_i^2}}e^{\\frac{-(x-\\mu_i)^2}{2\\sigma_i^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ \\Large P(y|x_1, x_2, ..., x_n) = \\frac{P(y)\\prod_{i}^{n}P(x_i|y)}{P(x_1, x_2, ..., x_n)}$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-03T15:28:07.427Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aggs = df.groupby('Target').agg(['mean', 'std'])\n",
    "aggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-03T15:28:07.433Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "bayes = GaussianNB()\n",
    "bayes.fit(X_tr,y_tr)\n",
    "evaluate_classification(bayes,X_te,y_te,X_train=X_tr,y_train=y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-03T15:28:07.439Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def p_x_given_class(obs_row, feature, class_):\n",
    "    mu = aggs[feature]['mean'][class_]\n",
    "    std = aggs[feature]['std'][class_]\n",
    "    \n",
    "    # A single observation\n",
    "    obs = df.iloc[obs_row][feature] \n",
    "    \n",
    "    p_x_given_y = stats.norm.pdf(obs, loc=mu, scale=std)\n",
    "    return p_x_given_y\n",
    "\n",
    "# Notice how this is not a true probability; you can get values > 1\n",
    "p_x_given_class(0, 'petal length (cm)', 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-03T15:28:07.444Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "row = 100\n",
    "c_probs = []\n",
    "for c in range(3):\n",
    "    # Initialize probability to relative probability of class \n",
    "    p = len(df[df['Target'] == c])/len(df) \n",
    "    for feature in X.columns:\n",
    "        p *= p_x_given_class(row, feature, c) \n",
    "        # Update the probability using the point estimate for each feature\n",
    "        c_probs.append(p)\n",
    "\n",
    "c_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-03T15:28:07.449Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def predict_class(row):\n",
    "    c_probs = []\n",
    "    for c in range(3):\n",
    "        # Initialize probability to relative probability of class\n",
    "        p = len(df[df['Target'] == c])/len(df) \n",
    "        for feature in X.columns:\n",
    "            p *= p_x_given_class(row, feature, c)\n",
    "        c_probs.append(p)\n",
    "    return np.argmax(c_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-03T15:28:07.455Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "row = 0\n",
    "df.iloc[row]\n",
    "predict_class(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-03T15:28:07.460Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['Predictions'] =  [predict_class(row) for row in df.index]\n",
    "df['Correct?'] = df['Target'] == df['Predictions']\n",
    "df['Correct?'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Avoiding \"underflow\"\n",
    "\n",
    "> \"...repeatedly multiplying small probabilities can lead to underflow; rounding to zero due to numerical approximation limitations. As such, a common alternative is to add the logarithms of the probabilities as opposed to multiplying the raw probabilities themselves...\"<br>\n",
    "$$ \\large e^x \\cdot e^y = e^{x+y}$$  \n",
    "$$ \\large log_{e}(e)=1 $$  \n",
    "$$\\large  e^{log(x)} = x$$ \n",
    "\n",
    "With that, here's an updated version of the function using log probabilities to avoid underflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-03T15:28:07.465Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def predict_class_log(row):\n",
    "    c_probs = []\n",
    "    for c in range(3):\n",
    "        # Initialize probability to relative probability of class\n",
    "        p = len(df[df['Target'] == c])/len(df) \n",
    "        for feature in X.columns:\n",
    "            p += np.log(p_x_given_class(row, feature, c))\n",
    "        c_probs.append(p)\n",
    "    return np.argmax(c_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-03T15:28:07.470Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row = 0\n",
    "\n",
    "df.iloc[row]\n",
    "print(predict_class_log(row))\n",
    "df['Predictions'] =  [predict_class_log(row) for row in df.index]\n",
    "df['Correct?'] = df['Target'] == df['Predictions']\n",
    "df['Correct?'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env-new",
   "language": "python",
   "name": "learn-env-new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0cd25ab07e664e839a42b9ac2fa6ad99": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1989e13775384856905bd5e8dac53607": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [
        "widget-interact"
       ],
       "children": [
        "IPY_MODEL_3609ecab94854243bf79cb6c88be6df3",
        "IPY_MODEL_50da50b1cf844dc19c5b14f6bbe62c88"
       ],
       "layout": "IPY_MODEL_cc1c3e4707244fba8956fb015f6c74c3"
      }
     },
     "269bf34c50e742acbef263e949284945": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3609ecab94854243bf79cb6c88be6df3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntSliderModel",
      "state": {
       "description": "i",
       "layout": "IPY_MODEL_0cd25ab07e664e839a42b9ac2fa6ad99",
       "max": 14065,
       "style": "IPY_MODEL_4043f03f151c4f6e9f347a05645b281b",
       "value": 7032
      }
     },
     "3b8d9ff671cc4b8cb711bd4259467459": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_269bf34c50e742acbef263e949284945",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "Comey testified (under oath) that it was a “unanimous” decision on Crooked Hillary. Lisa Page transcripts show he LIED. @jasoninthehouse\n['comey', 'testified', 'oath', 'unanimous', 'decision', 'crooked', 'hillary', 'lisa', 'page', 'transcripts', 'show', 'lied', 'jasoninthehouse']\n['comey', 'testified', 'oath', 'unanimous', 'decision', 'crooked', 'hillary', 'lisa', 'page', 'transcripts', 'show', 'lied', 'jasoninthehouse']\n"
        }
       ]
      }
     },
     "3f8a7f7f6ca84d909af45f28ade6e250": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4043f03f151c4f6e9f347a05645b281b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "50da50b1cf844dc19c5b14f6bbe62c88": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_3f8a7f7f6ca84d909af45f28ade6e250",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "- Tweet #7032:\n\nComey testified (under oath) that it was a “unanimous” decision on Crooked Hillary. Lisa Page transcripts show he LIED. @jasoninthehouse \n\n['Comey', 'testified', '(', 'under', 'oath', ')', 'that', 'it', 'was', 'a', '“', 'unanimous', '”', 'decision', 'on', 'Crooked', 'Hillary', '.', 'Lisa', 'Page', 'transcripts', 'show', 'he', 'LIED', '.', '@', 'jasoninthehouse']\n\n['comey', 'testified', 'oath', 'unanimous', 'decision', 'crooked', 'hillary', 'lisa', 'page', 'transcripts', 'show', 'lied', 'jasoninthehouse']\n"
        }
       ]
      }
     },
     "60024863b7e24c3f886103a999a60fb2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7328dc93e0534ed892df5a9d4431a28f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "93231fb165ca4b02a9a13fef729e4071": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "cc1c3e4707244fba8956fb015f6c74c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "eacdf6995a6b410d9a381da547d8f674": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntSliderModel",
      "state": {
       "description": "i",
       "layout": "IPY_MODEL_60024863b7e24c3f886103a999a60fb2",
       "max": 14065,
       "style": "IPY_MODEL_93231fb165ca4b02a9a13fef729e4071",
       "value": 7032
      }
     },
     "f0e12d1387544d9a8381707889e119e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [
        "widget-interact"
       ],
       "children": [
        "IPY_MODEL_eacdf6995a6b410d9a381da547d8f674",
        "IPY_MODEL_3b8d9ff671cc4b8cb711bd4259467459"
       ],
       "layout": "IPY_MODEL_7328dc93e0534ed892df5a9d4431a28f"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
