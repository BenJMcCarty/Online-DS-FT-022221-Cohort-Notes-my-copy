{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zr65hBzg5Phe"
   },
   "source": [
    "### BONUS FUNCTIONS:\n",
    "- didn't get to in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T22:16:05.590997Z",
     "start_time": "2020-05-22T22:16:04.412Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "4RM3tQkqyuoe"
   },
   "outputs": [],
   "source": [
    "def get_all_links(soup):#,attr_kwds=None):\n",
    "    \"\"\"Finds all links inside of soup that have the attributes(attr_kwds),which will be used in soup.findAll(attrs=attr_kwds).\n",
    "    Returns a list of links.\n",
    "    tag_type = 'a' or 'href'\"\"\"\n",
    "    all_a_tags = soup.findAll('a',attrs=kwds) \n",
    "    link_list = []\n",
    "    for link in all_a_tags:\n",
    "        test_link = link.get('href')#,attr=kwds)\n",
    "#         test_link = link.get('href',attrs=kwds)\n",
    "        link_list.append(test_link)\n",
    "    return link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T22:16:05.592035Z",
     "start_time": "2020-05-22T22:16:04.414Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UNjG2XYFyPIH"
   },
   "outputs": [],
   "source": [
    "def make_absolute_links(source_url, rel_link_list):\n",
    "    \"\"\"Accepts the source_url for the source page of the rel_link_list and uses urljoin to return a list of valid absolute links.\"\"\"\n",
    "    \n",
    "    from urllib.parse import urlparse, urljoin\n",
    "\n",
    "    absolute_links=[]\n",
    "\n",
    "    # Create a for loop to loop through links and make absolute html paths\n",
    "    for link in rel_link_list:\n",
    "\n",
    "        # Get base url using a url pasers and the story_url at the beginning of the nb\n",
    "        abs_link = urljoin(source_url,link)    \n",
    "\n",
    "        #concatenate and append to a list \n",
    "        absolute_links.append(abs_link)\n",
    "    \n",
    "    return absolute_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eVfjE8RhHZq-"
   },
   "source": [
    "#### Ex Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9xBlb5NuyeAC"
   },
   "source": [
    "- `soup = cook_soup_from_url(url)`\n",
    "    - make a beautiful soup from url\n",
    "-`soup_links = get_all_links(soup)`\n",
    "    - get all links from soup and return as a list.\n",
    "    \n",
    "-  `absolute_links = make_absolute_links(url, soup_links) `\n",
    "    - use If `soup_links` are relative links that do not include the website domain and start with '../' instead of 'https://www... ').\n",
    "    - then can use the `absolute_links` to make new soups to continue searching for your desired content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T22:16:05.593049Z",
     "start_time": "2020-05-22T22:16:04.418Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UwR64_fYv3Up"
   },
   "outputs": [],
   "source": [
    "def cook_soup_from_url(url, parser='lxml',sleep_time=0):\n",
    "    \"\"\"Uses requests to retreive webpage and returns a BeautifulSoup made using lxml parser.\"\"\"\n",
    "    import requests\n",
    "    from time import sleep\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    sleep(sleep_time)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # check status of request\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Error: Status_code !=200.\\n status_code={response.status_code}')\n",
    "                        \n",
    "    c = response.content\n",
    "    # feed content into a beautiful soup using lxml\n",
    "    soup = BeautifulSoup(c,'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T22:16:05.594104Z",
     "start_time": "2020-05-22T22:16:04.421Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Zo1sJyNE6ZCb"
   },
   "outputs": [],
   "source": [
    "def cook_batch_of_soups(link_list, sleep_time=1): #,user_fun = extract_target_text):\n",
    "    \"\"\"Accepts a list of links to extract and save in a list of dictionaries of soups\n",
    "    with their relative url path as their key.\n",
    "    Set user_fun to None to just extract full soups without user_extract\"\"\"\n",
    "    from time import sleep\n",
    "    from urllib.parse import urlparse, urljoin\n",
    "\n",
    "    batch_of_soups = []\n",
    "    \n",
    "    for link in link_list:\n",
    "        soup_dict = {}\n",
    "        \n",
    "        \n",
    "        # turn the url path into the dictionary key/title\n",
    "        url_dict_key_path = urlparse(link).path\n",
    "        url_dict_key = url_dict_key_path.split('/')[-1]\n",
    "        \n",
    "        soup_dict['_url'] = link\n",
    "        soup_dict['path'] = url_dict_key\n",
    "\n",
    "        # make a soup from the current link\n",
    "        page_soup = cook_soup_from_url(link, sleep_time=sleep_time)\n",
    "        soup_dict['soup'] = page_soup\n",
    "\n",
    "        \n",
    "#         if user_fun!=None:\n",
    "#             ## ADDING USER-SPECIFIED EXTRACTION FUNCTION       \n",
    "#             user_output = user_fun(page_soup) #can add inputs to function\n",
    "#             soup_dict['user_extract'] = user_output\n",
    "        \n",
    "        # Add current page's soup to batch_of_soups list\n",
    "        batch_of_soups.append(soup_dict)\n",
    "        \n",
    "    return batch_of_soups\n",
    "\n",
    "\n",
    "def extract_target_text(soup_or_tag,tag_name='p', attrs_dict=None, join_text =True, save_files=False):\n",
    "    \"\"\"User-specified function to add extraction of specific content during 'cook batch of soups'\"\"\"\n",
    "    \n",
    "    if attrs_dict==None:\n",
    "        found_tags = soup_or_tag.find_all(name=tag_name)\n",
    "    else:\n",
    "        found_tags = soup_or_tag.find_all(name=tag_name,attrs=attrs_dict)\n",
    "    \n",
    "    \n",
    "    # if extracting from multiple tags\n",
    "    output=[]\n",
    "    output = [tag.text for tag in found_tags if tag.text is not None]\n",
    "    \n",
    "    if join_text == True:\n",
    "        output = ' '.join(output)\n",
    "\n",
    "    ## ADDING SAVING EACH \n",
    "    if save_files==True:\n",
    "        text = output #soup.body.string\n",
    "        filename =f\"drive/My Drive/text_extract_{url_dict_key}.txt\"\n",
    "        soup_dict['filename'] = filename\n",
    "        with open(filename,'w+') as f:\n",
    "            f.write(text)\n",
    "        print(f'File  successfully saved as {filename}')\n",
    "\n",
    "    return  output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T22:16:05.595216Z",
     "start_time": "2020-05-22T22:16:04.423Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "0ODwEjKH92T5"
   },
   "outputs": [],
   "source": [
    "def pickled_soup(soups, save_location='./', pickle_name='exported_soups.pckl'):\n",
    "    import pickle\n",
    "    import sys\n",
    "    \n",
    "    filepath = save_location+pickle_name\n",
    "    \n",
    "    with open(filepath,'wb') as f:\n",
    "        pickle.dump(soups, f)\n",
    "        \n",
    "    return print(f'Soup successfully pickled. Stored as {filepath}.')\n",
    "\n",
    "def load_leftovers(filepath):\n",
    "    import pickle\n",
    "    \n",
    "    print(f'Opening leftovers: {filepath}')\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        leftover_soup = pickle.load(f)\n",
    "        \n",
    "    return leftover_soup\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ng-HN_rRymjI"
   },
   "source": [
    "#### Walkthrough - using James' functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T22:16:05.596301Z",
     "start_time": "2020-05-22T22:16:04.426Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "1Pg3VWWmypij"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
    "soup = cook_soup_from_url(url,sleep_time=1)\n",
    "\n",
    "\n",
    "## Get all links that match are interal wikipedia redirects [yes?]\n",
    "kwds = {'class':'mw-redirect'}\n",
    "links = get_all_links(soup)#,kwds)\n",
    "\n",
    "\n",
    "# preview first 5 links\n",
    "print(links[:5])\n",
    "\n",
    "# Turn relative links into absolute links\n",
    "abs_links = make_absolute_links(url,links)\n",
    "print(abs_links[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T22:16:05.597455Z",
     "start_time": "2020-05-22T22:16:04.429Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "6tCwXLiN7UCk"
   },
   "outputs": [],
   "source": [
    "# Selecting only the first 5 links to test\n",
    "abs_links_for_soups = abs_links[:5]\n",
    "\n",
    "\n",
    "# Cooking a batch of soups from those chosen links\n",
    "batch_of_soups = cook_batch_of_soups(abs_links_for_soups, sleep_time=2)\n",
    "\n",
    "# batch_of_soups is a list as long as the input link_list\n",
    "print(f'# of input links: == # of soups in batch:\\n{len(abs_links_for_soups)} == {len(batch_of_soups)}\\n')\n",
    "\n",
    "# batch_of_soups is a list of soup-dictionaries\n",
    "soup_dict = batch_of_soups[0]\n",
    "print('Each soup_dict has ',soup_dict.keys())\n",
    "\n",
    "# the page's soup is stored under soup_dict['soup']\n",
    "soup_from_soup_dict = soup_dict['soup']\n",
    "type(soup_from_soup_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hFVnj20YmQK2"
   },
   "source": [
    "#### Notes on extracting content.\n",
    "- Edit the `extract_target_text function` in the James' functions settings or uncomment and use the `extract_target_text_custom function` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T22:16:05.598404Z",
     "start_time": "2020-05-22T22:16:04.431Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "QwU8aPHJhAVm"
   },
   "outputs": [],
   "source": [
    "## ADDING extract_target_text to precisely target text\n",
    "# def extract_target_text_custom(soup_or_tag,tag_name='p', attrs_dict=None, join_text =True, save_files=False):\n",
    "#     \"\"\"User-specified function to add extraction of specific content during 'cook batch of soups'\"\"\"\n",
    "    \n",
    "#     if attrs_dict==None:\n",
    "#         found_tags = soup_or_tag.find_all(name=tag_name)\n",
    "#     else:\n",
    "#         found_tags = soup_or_tag.find_all(name=tag_name,attrs=attrs_dict)\n",
    "    \n",
    "    \n",
    "#     # if extracting from multiple tags\n",
    "#     output=[]\n",
    "#     output = [tag.text for tag in found_tags if tag.text is not None]\n",
    "    \n",
    "#     if join_text == True:\n",
    "#         output = ' '.join(output)\n",
    "\n",
    "#     ## ADDING SAVING EACH \n",
    "#     if save_files==True:\n",
    "#         text = output #soup.body.string\n",
    "#         filename =f\"drive/My Drive/text_extract_{url_dict_key}.txt\"\n",
    "#         soup_dict['filename'] = filename\n",
    "#         with open(filename,'w+') as f:\n",
    "#             f.write(text)\n",
    "#         print(f'File  successfully saved as {filename}')\n",
    "\n",
    "#     return  output\n",
    "\n",
    "# ####################\n",
    "\n",
    "## RUN A LOOP TO ADD EXTRACTED TEXT TO EACH SOUP IN THE BATCH\n",
    "for i, soup_dict in enumerate(batch_of_soups):\n",
    "    \n",
    "    # Get the soup from the dict\n",
    "    soup = soup_dict['soup']\n",
    "    \n",
    "    # Extract text \n",
    "    extracted_text = extract_target_text(soup)\n",
    "    \n",
    "    # Add key:value for results of extract\n",
    "    soup_dict['extracted'] = extracted_text\n",
    "    \n",
    "    # Replace the old soup_dict with the new one with 'extracted'\n",
    "    batch_of_soups[i] = soup_dict\n",
    "    \n",
    "example_extracted_text=batch_of_soups[0]['extracted']\n",
    "print(example_extracted_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T22:16:05.599650Z",
     "start_time": "2020-05-22T22:16:04.433Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "eUJ50zdSvjyL"
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# from fake_useragent import UserAgent\n",
    "# ua = UserAgent()\n",
    "\n",
    "# header = {'user-agent':ua.chrome}\n",
    "# print('Header:\\n',header)\n",
    "\n",
    "# url ='https://en.wikipedia.org/wiki/Stock_market'\n",
    "# response = requests.get(url, timeout=3, headers=header)\n",
    "\n",
    "# print('Status code: ',response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BONUS FUNCTIONS:\n",
    "- didn't get to in class\n",
    "\n",
    "def get_all_links(soup):#,attr_kwds=None):\n",
    "    \"\"\"Finds all links inside of soup that have the attributes(attr_kwds),which will be used in soup.findAll(attrs=attr_kwds).\n",
    "    Returns a list of links.\n",
    "    tag_type = 'a' or 'href'\"\"\"\n",
    "    all_a_tags = soup.findAll('a',attrs=kwds) \n",
    "    link_list = []\n",
    "    for link in all_a_tags:\n",
    "        test_link = link.get('href')#,attr=kwds)\n",
    "#         test_link = link.get('href',attrs=kwds)\n",
    "        link_list.append(test_link)\n",
    "    return link_list\n",
    "\n",
    "def make_absolute_links(source_url, rel_link_list):\n",
    "    \"\"\"Accepts the source_url for the source page of the rel_link_list and uses urljoin to return a list of valid absolute links.\"\"\"\n",
    "    \n",
    "    from urllib.parse import urlparse, urljoin\n",
    "\n",
    "    absolute_links=[]\n",
    "\n",
    "    # Create a for loop to loop through links and make absolute html paths\n",
    "    for link in rel_link_list:\n",
    "\n",
    "        # Get base url using a url pasers and the story_url at the beginning of the nb\n",
    "        abs_link = urljoin(source_url,link)    \n",
    "\n",
    "        #concatenate and append to a list \n",
    "        absolute_links.append(abs_link)\n",
    "    \n",
    "    return absolute_links\n",
    "\n",
    "#### Ex Functions\n",
    "\n",
    "\n",
    "- `soup = cook_soup_from_url(url)`\n",
    "    - make a beautiful soup from url\n",
    "-`soup_links = get_all_links(soup)`\n",
    "    - get all links from soup and return as a list.\n",
    "    \n",
    "-  `absolute_links = make_absolute_links(url, soup_links) `\n",
    "    - use If `soup_links` are relative links that do not include the website domain and start with '../' instead of 'https://www... ').\n",
    "    - then can use the `absolute_links` to make new soups to continue searching for your desired content.\n",
    "\n",
    "\n",
    "def cook_soup_from_url(url, parser='lxml',sleep_time=0):\n",
    "    \"\"\"Uses requests to retreive webpage and returns a BeautifulSoup made using lxml parser.\"\"\"\n",
    "    import requests\n",
    "    from time import sleep\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    sleep(sleep_time)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # check status of request\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Error: Status_code !=200.\\n status_code={response.status_code}')\n",
    "                        \n",
    "    c = response.content\n",
    "    # feed content into a beautiful soup using lxml\n",
    "    soup = BeautifulSoup(c,'lxml')\n",
    "    return soup\n",
    "\n",
    "def cook_batch_of_soups(link_list, sleep_time=1): #,user_fun = extract_target_text):\n",
    "    \"\"\"Accepts a list of links to extract and save in a list of dictionaries of soups\n",
    "    with their relative url path as their key.\n",
    "    Set user_fun to None to just extract full soups without user_extract\"\"\"\n",
    "    from time import sleep\n",
    "    from urllib.parse import urlparse, urljoin\n",
    "\n",
    "    batch_of_soups = []\n",
    "    \n",
    "    for link in link_list:\n",
    "        soup_dict = {}\n",
    "        \n",
    "        \n",
    "        # turn the url path into the dictionary key/title\n",
    "        url_dict_key_path = urlparse(link).path\n",
    "        url_dict_key = url_dict_key_path.split('/')[-1]\n",
    "        \n",
    "        soup_dict['_url'] = link\n",
    "        soup_dict['path'] = url_dict_key\n",
    "\n",
    "        # make a soup from the current link\n",
    "        page_soup = cook_soup_from_url(link, sleep_time=sleep_time)\n",
    "        soup_dict['soup'] = page_soup\n",
    "\n",
    "        \n",
    "#         if user_fun!=None:\n",
    "#             ## ADDING USER-SPECIFIED EXTRACTION FUNCTION       \n",
    "#             user_output = user_fun(page_soup) #can add inputs to function\n",
    "#             soup_dict['user_extract'] = user_output\n",
    "        \n",
    "        # Add current page's soup to batch_of_soups list\n",
    "        batch_of_soups.append(soup_dict)\n",
    "        \n",
    "    return batch_of_soups\n",
    "\n",
    "\n",
    "def extract_target_text(soup_or_tag,tag_name='p', attrs_dict=None, join_text =True, save_files=False):\n",
    "    \"\"\"User-specified function to add extraction of specific content during 'cook batch of soups'\"\"\"\n",
    "    \n",
    "    if attrs_dict==None:\n",
    "        found_tags = soup_or_tag.find_all(name=tag_name)\n",
    "    else:\n",
    "        found_tags = soup_or_tag.find_all(name=tag_name,attrs=attrs_dict)\n",
    "    \n",
    "    \n",
    "    # if extracting from multiple tags\n",
    "    output=[]\n",
    "    output = [tag.text for tag in found_tags if tag.text is not None]\n",
    "    \n",
    "    if join_text == True:\n",
    "        output = ' '.join(output)\n",
    "\n",
    "    ## ADDING SAVING EACH \n",
    "    if save_files==True:\n",
    "        text = output #soup.body.string\n",
    "        filename =f\"drive/My Drive/text_extract_{url_dict_key}.txt\"\n",
    "        soup_dict['filename'] = filename\n",
    "        with open(filename,'w+') as f:\n",
    "            f.write(text)\n",
    "        print(f'File  successfully saved as {filename}')\n",
    "\n",
    "    return  output\n",
    "\n",
    "\n",
    "\n",
    "def pickled_soup(soups, save_location='./', pickle_name='exported_soups.pckl'):\n",
    "    import pickle\n",
    "    import sys\n",
    "    \n",
    "    filepath = save_location+pickle_name\n",
    "    \n",
    "    with open(filepath,'wb') as f:\n",
    "        pickle.dump(soups, f)\n",
    "        \n",
    "    return print(f'Soup successfully pickled. Stored as {filepath}.')\n",
    "\n",
    "def load_leftovers(filepath):\n",
    "    import pickle\n",
    "    \n",
    "    print(f'Opening leftovers: {filepath}')\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        leftover_soup = pickle.load(f)\n",
    "        \n",
    "    return leftover_soup\n",
    "        \n",
    "\n",
    "#### Walkthrough - using James' functions\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
    "soup = cook_soup_from_url(url,sleep_time=1)\n",
    "\n",
    "\n",
    "## Get all links that match are interal wikipedia redirects [yes?]\n",
    "kwds = {'class':'mw-redirect'}\n",
    "links = get_all_links(soup)#,kwds)\n",
    "\n",
    "\n",
    "# preview first 5 links\n",
    "print(links[:5])\n",
    "\n",
    "# Turn relative links into absolute links\n",
    "abs_links = make_absolute_links(url,links)\n",
    "print(abs_links[:5])\n",
    "\n",
    "# Selecting only the first 5 links to test\n",
    "abs_links_for_soups = abs_links[:5]\n",
    "\n",
    "\n",
    "# Cooking a batch of soups from those chosen links\n",
    "batch_of_soups = cook_batch_of_soups(abs_links_for_soups, sleep_time=2)\n",
    "\n",
    "# batch_of_soups is a list as long as the input link_list\n",
    "print(f'# of input links: == # of soups in batch:\\n{len(abs_links_for_soups)} == {len(batch_of_soups)}\\n')\n",
    "\n",
    "# batch_of_soups is a list of soup-dictionaries\n",
    "soup_dict = batch_of_soups[0]\n",
    "print('Each soup_dict has ',soup_dict.keys())\n",
    "\n",
    "# the page's soup is stored under soup_dict['soup']\n",
    "soup_from_soup_dict = soup_dict['soup']\n",
    "type(soup_from_soup_dict)\n",
    "\n",
    "#### Notes on extracting content.\n",
    "- Edit the `extract_target_text function` in the James' functions settings or uncomment and use the `extract_target_text_custom function` below\n",
    "\n",
    "## ADDING extract_target_text to precisely target text\n",
    "# def extract_target_text_custom(soup_or_tag,tag_name='p', attrs_dict=None, join_text =True, save_files=False):\n",
    "#     \"\"\"User-specified function to add extraction of specific content during 'cook batch of soups'\"\"\"\n",
    "    \n",
    "#     if attrs_dict==None:\n",
    "#         found_tags = soup_or_tag.find_all(name=tag_name)\n",
    "#     else:\n",
    "#         found_tags = soup_or_tag.find_all(name=tag_name,attrs=attrs_dict)\n",
    "    \n",
    "    \n",
    "#     # if extracting from multiple tags\n",
    "#     output=[]\n",
    "#     output = [tag.text for tag in found_tags if tag.text is not None]\n",
    "    \n",
    "#     if join_text == True:\n",
    "#         output = ' '.join(output)\n",
    "\n",
    "#     ## ADDING SAVING EACH \n",
    "#     if save_files==True:\n",
    "#         text = output #soup.body.string\n",
    "#         filename =f\"drive/My Drive/text_extract_{url_dict_key}.txt\"\n",
    "#         soup_dict['filename'] = filename\n",
    "#         with open(filename,'w+') as f:\n",
    "#             f.write(text)\n",
    "#         print(f'File  successfully saved as {filename}')\n",
    "\n",
    "#     return  output\n",
    "\n",
    "# ####################\n",
    "\n",
    "## RUN A LOOP TO ADD EXTRACTED TEXT TO EACH SOUP IN THE BATCH\n",
    "for i, soup_dict in enumerate(batch_of_soups):\n",
    "    \n",
    "    # Get the soup from the dict\n",
    "    soup = soup_dict['soup']\n",
    "    \n",
    "    # Extract text \n",
    "    extracted_text = extract_target_text(soup)\n",
    "    \n",
    "    # Add key:value for results of extract\n",
    "    soup_dict['extracted'] = extracted_text\n",
    "    \n",
    "    # Replace the old soup_dict with the new one with 'extracted'\n",
    "    batch_of_soups[i] = soup_dict\n",
    "    \n",
    "example_extracted_text=batch_of_soups[0]['extracted']\n",
    "print(example_extracted_text[:1000])\n",
    "\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# from fake_useragent import UserAgent\n",
    "# ua = UserAgent()\n",
    "\n",
    "# header = {'user-agent':ua.chrome}\n",
    "# print('Header:\\n',header)\n",
    "\n",
    "# url ='https://en.wikipedia.org/wiki/Stock_market'\n",
    "# response = requests.get(url, timeout=3, headers=header)\n",
    "\n",
    "# print('Status code: ',response.status_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env-new",
   "language": "python",
   "name": "learn-env-new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
