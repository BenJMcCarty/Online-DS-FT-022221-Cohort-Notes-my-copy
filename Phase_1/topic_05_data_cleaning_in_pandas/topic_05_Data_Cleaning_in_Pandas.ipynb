{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 05: Data Cleaning in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- onl01-dtsc-ft-022221FT\n",
    "- 03/02/21\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "#### Our goals today are to be able to use the pandas library to:\n",
    "\n",
    "- Get summary info about a dataset and its variables\n",
    "  - Apply and use info, describe and dtypes\n",
    "  - Use mean, min, max, and value_counts \n",
    "- Use apply and applymap to transform columns and create new values\n",
    "\n",
    "- Explain lambda functions and use them to use an apply on a DataFrame\n",
    "- Explain what a groupby object is and split a DataFrame using a groupby\n",
    "- Reshape a DataFrame using joins, merges, pivoting, stacking, and melting\n",
    "--->\n",
    "\n",
    "- Identify missing values in a dataframe using built-in methods \n",
    "- Explain why missing values are a problem in data science \n",
    "- Evaluate and execute the best strategy for dealing with missing, duplicate, and erroneous values for a given dataset \n",
    "\n",
    "- Activity: Data Cleaning in Pandas Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Walk through data cleaning with pandas notebook\n",
    "- Walk through Topic 04's Ames Housing mini project\n",
    "- Walk through Topic 05's Mini-Project with Superheroes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions/Comments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the Gdoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When should we use map(), apply(), mapapply()? I'm having a hard time telling how they're different.\n",
    "\n",
    "\n",
    "- One of the solutions in the more data mapping lab included the following code to print the mean, median, and stddev of the age column. \n",
    "    - How was it possible to run functions from a list of strings representing the function names and apply()?\n",
    "```python\n",
    "age_na_mean = df['Age'].fillna(value=df['Age'].mean())\n",
    "print(age_na_mean.apply(['mean', 'median', 'std']))\n",
    "```\n",
    "- Can you explain more about how we can use the \"temporary\" changes on dataframes to our advantage?\n",
    "    - If we don't re-assign to a variable, or use inplace=True, what is the scope of the temporary changes, such as how long do they last?\n",
    "\n",
    "\n",
    "- The data cleaning mini project's data set (superheroes) had a decent amount of negative values for heights and weights that it appears were not taken out in the solution. I replaced these with medians, and got really weird displots for the gender-height-weight charts, although the regular hist() plots were less crazy looking. \n",
    "    - Can you go over displots a little more, and why we might use them instead of the other ways to do plots?\n",
    "    \n",
    "    \n",
    "    \n",
    "- Can you explain stack() and unstack a bit more? I ran through the lab examples but didn't really get what was happening\n",
    "\n",
    "- On this lab: https://github.com/learn-co-curriculum/dsc-combining-dataframes-pandas-lab/tree/solution When setting the index, how does join know to join ‘on’ the index (Primary Key as the lesson describes it)\n",
    "\n",
    "\n",
    "- I understand the value of inspecting dataframes with pandas - isolating data with .head(), .tail(), inspecting individual columns, etc, but are there any visual inspection tools that we will be able to use in conjunction with python similar to viewing the dataframe in excel? - Johnny D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lab 1 > Mini-Project - EDA with Pandas Using the Ames Housing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Folder `Online-DS-FT-022221-Cohort-Notes`>`Phase_1`>`topic_05_data_cleaning_in_pandas`>`labs_from_class`>`dsc-project-eda-with-pandas-master`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data Science Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## OSEMiN (Rhymes with Possum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "<img src='https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/images/OSEMN.png' width=800>\n",
    "<center><a href=\"https://www.kdnuggets.com/2018/02/data-science-command-line-book-exploring-data.html\"> \n",
    "    </a></center>\n",
    "\n",
    "\n",
    "> The Data Science Process we'll be using during this section--OSEMiN (pronounced \"OH-sum\", rhymes with \"possum\").  This is the most straightforward of the Data Science Processes discussed so far.  **Note that during this process, just like the others, the stages often blur together.***  It is completely acceptable (and ***often a best practice!) to float back and forth** between stages as you learn new things about your problem, dataset, requirements, etc.  \n",
    "It's quite common to get to the modeling step and realize that you need to scrub your data a bit more or engineer a different feature and jump back to the \"Scrub\" stage, or go all the way back to the \"Obtain\" stage when you realize your current data isn't sufficient to solve this problem. \n",
    "As with any of these frameworks, *OSEMiN is meant to be treated as guidelines, not law. \n",
    "</font>\n",
    "\n",
    "\n",
    "### OSEMN DETAILS\n",
    "\n",
    "**OBTAIN**\n",
    "\n",
    "- This step involves understanding stakeholder requirements, gathering information on the problem, and finally sourcing data that we think will be necessary for solving this problem. \n",
    "\n",
    ">**SCRUB**\n",
    "- During this stage, we'll focus on preprocessing our data.  **Important steps such as identifying and removing null values, dealing with outliers, normalizing data, and feature engineering/feature selection are handled around this stage.** The line with this stage really blurs with the _Explore_ stage, as it is common to only realize that certain columns require cleaning or preprocessing as a result of the visualzations and explorations done during Step 3.  \n",
    "- Note that although technically, categorical data should be one-hot encoded during this step, in practice, it's usually done after data exploration.  This is because it is much less time-consuming to visualize and explore a few columns containing categorical data than it is to explore many different dummy columns that have been one-hot encoded. \n",
    "\n",
    "**EXPLORE**\n",
    "\n",
    "- This step focuses on getting to know the dataset you're working with. As mentioned above, this step tends to blend with the _Scrub_ step mentioned above.  During this step, you'll create visualizations to really get a feel for your dataset.  You'll focus on things such as understanding the distribution of different columns, checking for multicollinearity, and other tasks liek that.  If your project is a classification task, you may check the balance of the different classes in your dataset.  If your problem is a regression task, you may check that the dataset meets the assumptions necessary for a regression task.  \n",
    "\n",
    "- At the end of this step, you should have a dataset ready for modeling that you've thoroughly explored and are extremely familiar with.  \n",
    "\n",
    "**MODEL**\n",
    "... Modeling with begin in Phase 2.\n",
    "<!---\n",
    "- This step, as with the last two frameworks, is also pretty self-explanatory. It consists of building and tuning models using all the tools you have in your data science toolbox.  In practice, this often means defining a threshold for success, selecting machine learning algorithms to test on the project, and tuning the ones that show promise to try and increase your results.  As with the other stages, it is both common and accepted to realize something, jump back to a previous stage like _Scrub_ or _Explore_, and make some changes to see how it affects the model.  \n",
    "\n",
    "**iNTERPRET**\n",
    "\n",
    "- During this step, you'll interpret the results of your model(s), and communicate results to stakeholders.  As with the other frameworks, communication is incredibily important! During this stage, you may come to realize that further investigation is needed, or more data.  That's totally fine--figure out what's needed, go get it, and start the process over! If your results are satisfactory to all stakeholders involved, you may also go from this stage right into productionizing your model and automating processes necessary to support it.  \n",
    "\n",
    "\n",
    "\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## PROCESS CHECKLIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### **OBTAIN**\n",
    "- Import data, inspect, check for datatypes to convert and null values\n",
    "- Display header and info.\n",
    "- Drop any unneeded columns, if known (`df.drop(['col1','col2'],axis=1,inplace=True`)\n",
    "\n",
    "\n",
    "\n",
    "#### **SCRUB**\n",
    "- Recast data types, identify outliers, ~~check for multicollinearity, normalize data~~(don't worry about these, for now)\n",
    "\n",
    "- [ ] Check for #'s that are store as objects (`df.info()`,`df.describe()`)\n",
    "    - when converting to #'s, look for odd values (like many 0's), or strings that can't be converted.\n",
    "    - Decide how to deal weird/null values (`df.unique()`, `df.isna().sum()`)\n",
    "    - `df.fillna(subset=['col_with_nulls'],'fill_value')`, `df.replace()`\n",
    "    \n",
    "    \n",
    "- [ ] Check for categorical variables stored as integers.\n",
    "        - May be easier to tell when you make a scatter plotm or `pd.plotting.scatter_matrix()`\n",
    "\n",
    "\n",
    "\n",
    "- [ ] Check for missing values  (df.isna().sum())\n",
    "    - Can drop rows or colums\n",
    "    - For missing numeric data with median or bin/convert to categorical\n",
    "    - For missing categorical data: make NaN own category OR replace with most common category\n",
    "    \n",
    "    \n",
    "  \n",
    "#### **EXPLORE**\n",
    "- [ ] Check distributions, outliers, etc**\n",
    "- [ ] Check scales, ranges (df.describe())\n",
    "- [ ] Check histograms to get an idea of distributions (df.hist()) and data transformations to perform.\n",
    "    - Can also do kernel density estimates\n",
    "- [ ] Use scatter plots to check for linearity and possible categorical variables (`df.plot(\"x\",\"y\")`)\n",
    "    - categoricals will look like vertical lines\n",
    "- [ ] Use `pd.plotting.scatter_matrix(df)` to visualize possible relationships\n",
    "- [ ] Check for linearity.\n",
    "   \n",
    "<!---   \n",
    "4. **[MODEL](#MODEL)**\n",
    "\n",
    "    - **Fit an initial model:** \n",
    "        - Run an initial model and get results\n",
    "\n",
    "    - **Holdout validation / Train/test split**\n",
    "        - use sklearn `train_test_split`\n",
    "    \n",
    "    \n",
    "5. **[iNTERPRET](#iNTERPRET)**\n",
    "    - **Assessing the model:**\n",
    "        - Assess parameters (slope,intercept)\n",
    "        - Check if the model explains the variation in the data (RMSE, F, R_square)\n",
    "        - *Are the coeffs, slopes, intercepts in appropriate units?*\n",
    "        - *Whats the impact of collinearity? Can we ignore?*\n",
    "        <br><br>\n",
    "    - **Revise the fitted model**\n",
    "        - Multicollinearity is big issue for lin regression and cannot fully remove it\n",
    "        - Use the predictive ability of model to test it (like R2 and RMSE)\n",
    "        - Check for missed non-linearity\n",
    "        \n",
    "       \n",
    "6. **Interpret final model and draw >=3 conclusions and recommendations from dataset**\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Dealing with Missing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Why is missing data a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Missing data can be problematic during the Data Science process because `NaN` values in the dataset limit our ability to do important things like:\n",
    "* Convert data types\n",
    "* Calculate summary statistics\n",
    "* Visualize data\n",
    "* Build models\n",
    "\n",
    "> Later on in the boot camp, we will be building machine learning models, which generally do NOT accept null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Detecting missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### `NaN`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "By default, pandas represents null values with `NaN`, which is short for **_Not a Number_**.  Pandas provides many great ways for checking for null values, built right into DataFrames and Series objects.\n",
    "\n",
    "#### Detecting `NaN`s\n",
    "\n",
    "```python\n",
    "df.isna()\n",
    "```\n",
    "```python\n",
    "df.isna().sum()\n",
    "```\n",
    "\n",
    "#### Creating Null Values\n",
    "\n",
    "- The proper way to create a null value is to use numpy's nan (`np.nan`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Placeholder values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Numerical data\n",
    "\n",
    "Numerical columns will often represent missing values with a value that is nonsensical to the column in question.  For instance, in healthcare data, missing values in a `Weight` column may be using impossible values such as `0` or `9999`.  These are valid to the computer, since they are real-numbered, but are obvious to anyone analyzing the data as placeholder values.  \n",
    "<!---\n",
    "- **Standard deviation**: If the data is normally distributed (or nearly normal), you can use three standard deviations as a cutoff point. In a normal distribution, three standard devations from the mean in both the directions cover 99.7% of the data, so any values outside this range are highly improbable, and can be safely discarded as outliers. \n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-dealing-missing-data-onl01-dtsc-pt-041320/master/images/normal_sd_new.png\" width=\"600\">\n",
    "\n",
    "\n",
    "> You will learn more about normal distribution in a later lesson. \n",
    "\n",
    "\n",
    "- **Interquartile range (IQR)**: If the data is not normally distribued, you can use the same method boxplots use to determine the outliers -- all observations that lie 1.5 times the IQR (difference between the 75th and the 25th percentiles of the data) away from the median in either direction are treated as outliers. \n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-dealing-missing-data-onl01-dtsc-pt-041320/master/images/new_boxplot.png\" width=\"600\">\n",
    "\n",
    "\n",
    "> If you need a refresher on IQR, refer to the lesson on _Measures of Dispersion_ in the _Importing and Statistical Analysis of Data_ section of Module 1.\n",
    "\n",
    "Another way to confirm these values is to check the `.value_counts()` of a column.  In a continuously-valued column, it is probably rare for one specific value to overwhelm all the others.  If, for instance, you see the same numerical value showing up a statistically improbable number of times, double-check that this value is real -- placeholder values have the potential to show up many times, but it's much less likely for real-valued numbers.  \n",
    "\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Categorical data\n",
    "\n",
    "To detect placeholder values in categorical data, get the unique values in the column and see if there are any values that don't match up with your expectations.  Pandas provides a built-in method for this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Strategies for dealing with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The easiest way to deal with missing values is to drop the offending rows and/or columns.  The downside to this is that we lose data in the process. \n",
    "\n",
    "- Drop columns\n",
    "\n",
    "- Dropping rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can also deal with missing values by replacing them with a common value. The downside of this method is that this can introduce noise into our dataset. \n",
    "\n",
    "- Continuous data\n",
    "    - For continuous data, the best solution is to replace the missing values with the median value for that column. \n",
    "    \n",
    "\n",
    "- Categorical data\n",
    "    - If one categorical value is much more common than others, it is a valid strategy to replace missing values with this common value. \n",
    "    - However, make sure to examine your data first! \n",
    "    - If all the categorical values are equally common, picking one to replace all the missing values may do more harm than good by skewing the distribution and introducing some false signal into your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Keep "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Sometimes, the knowledge that a value is missing can itself be informative for us.  If knowing that a value is missing tells you something, then it is often worth keeping the missing values using the following strategies. \n",
    "\n",
    "- Categorical data\n",
    "    - Just treat missing values as its own category! \n",
    "    - In that case, just replace the `NaN` values with the string `'NaN'`, or another string that makes it obvious that this value is `'missing'`.\n",
    "\n",
    "- Numerical data\n",
    "    - Leaving the `NaN`s alone isn't usually an option here. \n",
    "    - Instead, consider using **_Coarse Classification_**, also referred to as **_Binning_**.  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Additional Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Check for duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Check for repeated entries of the same row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Check for extraneous values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T22:41:13.268442Z",
     "start_time": "2020-05-04T22:41:13.264824Z"
    },
    "hidden": true
   },
   "source": [
    "In general, doing a quick eyeball and previewing the top occurring values for each feature can help further tease out peculiarities in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lab 2: Mini-Project - Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Folder `Online-DS-FT-022221-Cohort-Notes`>`Phase_1`>`topic_05_data_cleaning_in_pandas`>`labs_from_class`>`dsc-data-cleaning-project-master`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
