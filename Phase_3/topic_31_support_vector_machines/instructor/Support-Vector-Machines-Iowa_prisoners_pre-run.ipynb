{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "_Adapted from Yish's interpretation of Chapter 9 of Introduction to Statistical Learning in R_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "SVMs approach the classification problem in a direct way - __we try and find a plane that separates the classes in the feature space__\n",
    "\n",
    "If we cannot, we do one of the two things:\n",
    "\n",
    "- We __soften__ what we mean by \"separates\" \n",
    "- We __enlarge__ the feature space so that the separation is possible\n",
    "\n",
    "__Notes on Terminology__:\n",
    "\n",
    "- Support vector machines are sometimes used as a general method that incorporate maximal margin classifier, support vector classifiers etc. However, strictly by definition, support vector machine is a support vector classifier utilized with non-linear kernel. \n",
    "\n",
    "> \"When the support vector classifier is combined with a non-linear kernel such as (9.22), the resulting classifier is known as a support vector machine.\" -- P366, ISLR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a boundary hyperplane in a two dimensional space:\n",
    "\n",
    "<img src=\"../images/exampleboundary.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximal Margin Classifier\n",
    "\n",
    "SVM tackles the problem of classification directly, in the sense that it does not compute a probabilistic model. Instead, it constructs a hyperplane to directly separate the classes. \n",
    "\n",
    "For example:\n",
    "\n",
    "![](images/manyboundaries.png)\n",
    "\n",
    "However, the problem with this approach is that we can come up with infinite number of such hyperplanes as we can tilt the line back and forth and it would still serve the same purpose.\n",
    "\n",
    "__Therefore, we are using the hyperplane such that the it would be the farthest from training observations from either side__. The intuition behind it is that if we have chosen a hyperplane that is far from the training observations, it would be far for the testing observations as well. <br>\n",
    "\n",
    "The distance between the training observations and the hyperplane is called the _margin_, and the classifier aims to find the maximal margin from the hyperplane that separates the training examples:\n",
    "\n",
    "<img src=\"images/bestboundary.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Margin Classifier\n",
    "\n",
    "Even though the maximal margin classifier sounds like an intuitive idea and not too difficult to optimize for, it might not be desirable under two circumstances:\n",
    "\n",
    "1. It will be sensitive to individual training observations\n",
    "2. The algorithm will not converge if the training observations cannot be linearly separated.\n",
    "\n",
    "![](images/softboundary.png)\n",
    "\n",
    "What happens if we cannot come up with a hyperplane that perfectly separates the training observations like the ones above? The first solution is the soft margin classifier, where we can loosen up our definition of the margin. \n",
    "\n",
    "<img src=\"images/withoutboundary.png\" width=500>\n",
    "\n",
    "__Rather than seeking the largest possible margin so that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin, we instead allow some observations to be on the incorrect side of the margin, or even on the incorrect side of the hyperplane.__\n",
    "\n",
    "![](images/errortolerance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the hyperparameter Îµ is known as the slack variables, which dictate how many training observations we allow to violate the rule of margins or even the hyperplane. The amount of slack is bounded by C accordingly.\n",
    "\n",
    "The parameter Îµi tells us where the ith training observation is located. \n",
    "- If Îµi = 0, then we say the ith training observation is on the correct side of the margin;\n",
    "- If Îµi > 0, then we say it has violated the margin\n",
    "- If Îµi > 1, then it is on the wrong side of the hyperplane\n",
    "\n",
    "The value C is the sum of Îµ across all i training observations. The parameter C controls the bias-variance tradeoff of the statistical technique. A high value of C meaning we are more tolerant of the violation, which in turn might give us a model that has high bias but low variance; however, a low value of C indicates low tolerance of the violation, potentially giving us more variance but less bias. \n",
    "\n",
    "__How do we determine the ideal value of C?__\n",
    "\n",
    "![](images/impactofchyperparameter.png)\n",
    "\n",
    "__Note!__\n",
    "\n",
    "In scikit-learn implementation, `c` is defined as the inverse. A higher value of `c` is a smaller regularization or smaller penalty, whereas a lower value of `c` is a higher penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note!__\n",
    "It is important to point out that in the support vector classfier (or SVM) in general, only the vectors on the margins are used for classification. They are called the __\"Support Vectors\"__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"The Kernel Trick\"\n",
    "\n",
    "Sometimes we have training data that are not able to be separated even with softened margin:\n",
    "\n",
    "![](images/needforkerneltrick.png)\n",
    "\n",
    "The intuition to find the optimal fit is called feature space expansion:\n",
    "\n",
    "- First, we __enlarge__ the feature space through the use of kernel\n",
    "- Fit a support vector classifier in the enlarged space \n",
    "- This results in nonlinear decision boundaries in the original space \n",
    "\n",
    "\n",
    "<img src=\"images/nonlinearring.png\" width=500>\n",
    "\n",
    "<img src=\"images/nonlinearringin3d.png\" width=500>\n",
    "\n",
    "Why do we know that enlarging the feature space makes the data more linearly separable? [Cover's Theorem](https://en.wikipedia.org/wiki/Cover%27s_theorem).\n",
    "\n",
    "Another view:\n",
    "\n",
    "![](images/kernel_trick_hyperdimensional.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation & Performance Comparison\n",
    "\n",
    "Docs: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:08.088489Z",
     "start_time": "2021-05-13T15:17:07.016361Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Smaller Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:08.114503Z",
     "start_time": "2021-05-13T15:17:08.090942Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## ORIGINAL DATA\n",
    "\n",
    "df0 = pd.read_csv('../data/data_banknote_authentication.csv', header = None)\n",
    "\n",
    "# our data needs column names\n",
    "headers = [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\", \"Class\"]\n",
    "df0.columns = headers\n",
    "\n",
    "display(df0.head(),df0.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:08.131884Z",
     "start_time": "2021-05-13T15:17:08.117668Z"
    }
   },
   "outputs": [],
   "source": [
    "# define X and y, then train test split\n",
    "X0 = df0.drop('Class', axis=1)  \n",
    "y0 = df0['Class'] \n",
    "X_train0, X_test0, y_train0, y_test0 = train_test_split(X0, y0, test_size = 0.20) \n",
    "X_train0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“™ Predicting Recidivism with SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Iowa has a major problem with recidivism,  where ~38% of all inmates released from prison wind up back in jail after returning to a life of crime (AKA recidivism).\n",
    "- Dataset contains information on released prisoners and if they returned to prison within 3 years of being release. \n",
    "    - Dataset can be [found on Kaggle](https://www.kaggle.com/slonnadube/recidivism-for-offenders-released-from-prison), which was extracted from [Iowa's data portal](https://data.iowa.gov/Correctional-System/3-Year-Recidivism-for-Offenders-Released-from-Pris/mw8r-vqy4). \n",
    "\n",
    "- We will be using a partially pre-processed version of the dataset (columns have been renamed/simplified).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-3-final-project-online-ds-ft-021119/master/LSA_map_with_counties_districts_and_B54A5BBCE4156.jpg\" width=20%> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:08.136948Z",
     "start_time": "2021-05-13T15:17:08.134657Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# import functions_022221FT as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:09.896583Z",
     "start_time": "2021-05-13T15:17:08.139044Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-notebook')\n",
    "\n",
    "renamed_data = 'https://raw.githubusercontent.com/jirvingphd/dsc-3-final-project-online-ds-ft-021119/master/datasets/Iowa_Prisoners_Renamed_Columns_fsds_100719.csv'\n",
    "df = pd.read_csv(renamed_data,index_col=0)\n",
    "\n",
    "## Drop unwanted cols using year\n",
    "df = df.drop(columns=['yr_released','report_year'])\n",
    "display(df.head(),df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRUB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Null values (fill or drop)\n",
    "- Data Types (finding categorical variables)\n",
    "- Inspect the value_counts/labels of categoricals\n",
    "- Scaling or lack-off\n",
    "- One hot encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:09.901560Z",
     "start_time": "2021-05-13T15:17:09.899068Z"
    }
   },
   "outputs": [],
   "source": [
    "## Check null values\n",
    "# import missingno\n",
    "# missingno.matrix(df)\n",
    "# plt.show()\n",
    "# null_check = pd.DataFrame({\n",
    "#     '#null':df.isna().sum(),\n",
    "#     '%null':round(df.isna().sum()/len(df)*100,2)\n",
    "# })\n",
    "# null_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:09.905143Z",
     "start_time": "2021-05-13T15:17:09.903358Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ## inspect categories\n",
    "# dashes = '---'*20\n",
    "# for col in df.columns:\n",
    "#     print(dashes)\n",
    "#     print(f\"Value Counts for {col}:\")\n",
    "#     display(df[col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `race_ethnicity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:09.912314Z",
     "start_time": "2021-05-13T15:17:09.909813Z"
    }
   },
   "outputs": [],
   "source": [
    "# df['race_ethnicity'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Simplify race and ethnicity down to race\n",
    "    - (also tried separating race and ethnicity and using as 2 separate features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:09.927887Z",
     "start_time": "2021-05-13T15:17:09.915645Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining Dictionary Map for race_ethnicity categories\n",
    "race_ethnicity_map = {'White - Non-Hispanic':'White',\n",
    "                        'Black - Non-Hispanic': 'Black',\n",
    "                        'White - Hispanic' : 'Hispanic',\n",
    "                        'American Indian or Alaska Native - Non-Hispanic' : 'American Native',\n",
    "                        'Asian or Pacific Islander - Non-Hispanic' : 'Asian or Pacific Islander',\n",
    "                        'Black - Hispanic' : 'Black',\n",
    "                        'American Indian or Alaska Native - Hispanic':'American Native',\n",
    "                        'White -' : 'White',\n",
    "                        'Asian or Pacific Islander - Hispanic' : 'Asian or Pacific Islander',\n",
    "                        'N/A -' : np.nan,\n",
    "                        'Black -':'Black'}\n",
    "\n",
    "df['race'] = df['race_ethnicity'].map(race_ethnicity_map)\n",
    "df['race'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `crime_class`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:09.932163Z",
     "start_time": "2021-05-13T15:17:09.929898Z"
    }
   },
   "outputs": [],
   "source": [
    "# df['crime_class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After some research, we found that several of the less-frequent classes were actually equivalent to other classes. (e.g. 'Special Sentence 2005' -> 'Sex Offender',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:09.946051Z",
     "start_time": "2021-05-13T15:17:09.933917Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remapping\n",
    "crime_class_map = {'Other Felony (Old Code)': np.nan ,#or other felony\n",
    "                  'Other Misdemeanor':np.nan,\n",
    "                   'Felony - Mandatory Minimum':np.nan, # if minimum then lowest sentence ==  D Felony\n",
    "                   'Special Sentence 2005': 'Sex Offender',\n",
    "                   'Other Felony' : np.nan ,\n",
    "                   'Sexual Predator Community Supervision' : 'Sex Offender',\n",
    "                   'D Felony': 'D Felony',\n",
    "                   'C Felony' :'C Felony',\n",
    "                   'B Felony' : 'B Felony',\n",
    "                   'A Felony' : 'A Felony',\n",
    "                   'Aggravated Misdemeanor':'Aggravated Misdemeanor',\n",
    "                   'Felony - Enhancement to Original Penalty':'Felony - Enhanced',\n",
    "                   'Felony - Enhanced':'Felony - Enhanced' ,\n",
    "                   'Serious Misdemeanor':'Serious Misdemeanor',\n",
    "                   'Simple Misdemeanor':'Simple Misdemeanor'}\n",
    "\n",
    "df['crime_class'] = df['crime_class'].map(crime_class_map)\n",
    "df['crime_class'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `age_released`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:09.956219Z",
     "start_time": "2021-05-13T15:17:09.948164Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['age_released'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:09.963301Z",
     "start_time": "2021-05-13T15:17:09.958727Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mapping age_map onto 'age_released'\n",
    "# Encoding age groups as ordinal\n",
    "age_ranges = ('Under 25','25-34', '35-44','45-54','55 and Older')\n",
    "age_numbers = (20,30,40,50,65) \n",
    "\n",
    "age_num_map = dict(zip(age_ranges,age_numbers))\n",
    "age_num_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:09.975542Z",
     "start_time": "2021-05-13T15:17:09.965594Z"
    }
   },
   "outputs": [],
   "source": [
    "df['age_number'] = df['age_released'].map(age_num_map)\n",
    "df['age_number'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:09.979984Z",
     "start_time": "2021-05-13T15:17:09.977599Z"
    }
   },
   "outputs": [],
   "source": [
    "## saving list of features thaat have been replaced wiht engineered nes\n",
    "drop_cols = ['age_released','race_ethnicity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:10.003877Z",
     "start_time": "2021-05-13T15:17:09.982064Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['recidivist'].map({'Yes':1,\"No\":0})\n",
    "X = df.drop(columns=['recidivist',*drop_cols])\n",
    "\n",
    "## Train test split\n",
    "X_tr, X_te, y_tr,y_te  = train_test_split(X,y,test_size=.25)\n",
    "#                                                     random_state=3210)#,stratify=y)\n",
    "y_tr.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipelnes and ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:10.059422Z",
     "start_time": "2021-05-13T15:17:10.005738Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder,MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:10.072347Z",
     "start_time": "2021-05-13T15:17:10.061480Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tr.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:10.076334Z",
     "start_time": "2021-05-13T15:17:10.074144Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:10.086042Z",
     "start_time": "2021-05-13T15:17:10.078113Z"
    }
   },
   "outputs": [],
   "source": [
    "## saving list of numeric vs categorical feature\n",
    "num_cols = list(X_tr.select_dtypes('number').columns)\n",
    "cat_cols = list(X_tr.select_dtypes('object').columns)\n",
    "num_cols,cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:10.135734Z",
     "start_time": "2021-05-13T15:17:10.087911Z"
    }
   },
   "outputs": [],
   "source": [
    "## create pipelines and column transformer\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer',SimpleImputer(strategy='median')),\n",
    "    ('scale',MinMaxScaler())\n",
    "])\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer',SimpleImputer(strategy='constant',fill_value='MISSING')),\n",
    "    ('encoder',OneHotEncoder(sparse=False,handle_unknown='ignore'))])\n",
    "\n",
    "## COMBINE BOTH PIPELINES INTO ONE WITH COLUMN TRANSFORMER\n",
    "preprocessor=ColumnTransformer(transformers=[\n",
    "    ('num',num_transformer,num_cols),\n",
    "    ('cat',cat_transformer,cat_cols)])\n",
    "\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:10.306959Z",
     "start_time": "2021-05-13T15:17:10.137645Z"
    }
   },
   "outputs": [],
   "source": [
    "## Fit preprocessing pipeline on training data and pull out the feature names and X_cols\n",
    "preprocessor.fit(X_tr)\n",
    "\n",
    "## Use the encoder's .get_feature_names\n",
    "cat_features = list(preprocessor.named_transformers_['cat'].named_steps['encoder']\\\n",
    "                            .get_feature_names(cat_cols))\n",
    "X_cols = num_cols+cat_features\n",
    "\n",
    "## Transform X_traian,X_test and remake dfs\n",
    "X_train_df = pd.DataFrame(preprocessor.transform(X_tr),\n",
    "                          index=X_tr.index, columns=X_cols)\n",
    "X_test_df = pd.DataFrame(preprocessor.transform(X_te),\n",
    "                          index=X_te.index, columns=X_cols)\n",
    "\n",
    "## Tranform X_train and X_test and make into DataFrames\n",
    "X_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resampling with SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:10.426644Z",
     "start_time": "2021-05-13T15:17:10.313009Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE,SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:10.433014Z",
     "start_time": "2021-05-13T15:17:10.429551Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-13T15:17:10.437287Z",
     "start_time": "2021-05-13T15:17:10.434814Z"
    }
   },
   "outputs": [],
   "source": [
    "## Save list of trues and falses for each cols\n",
    "smote_feats = [False]*len(num_cols) +[True]*len(cat_features)\n",
    "# smote_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-13T15:17:07.092Z"
    }
   },
   "outputs": [],
   "source": [
    "smote = SMOTENC(smote_feats)\n",
    "X_train_sm,y_train_sm = smote.fit_sample(X_train_df,y_tr)\n",
    "y_train_sm.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“™ MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â«š Selecting the Final Dataset for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following cell determines which dataset is used for modeling. Modify the `PRISONERS` and `RESAMPLED_PRISONERS` correspondingly\n",
    "- If `PRISONERS` == False:\n",
    "    - Use original smaller bank-note dataset\n",
    "- If `PRISONERS` == True:\n",
    "    - If `RESAMPLED_PRISONERS` == False:\n",
    "        - Use original imbalanced prisoners dataset\n",
    "    - If `RESAMPLED_PRISONERS` == True:\n",
    "        - Use resampled training dataset.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-13T15:17:07.096Z"
    }
   },
   "outputs": [],
   "source": [
    "## Setting X_train/X_test\n",
    "PRISONERS = True\n",
    "RESAMPLED_PRISONERS = True\n",
    "\n",
    "\n",
    "if PRISONERS==False:\n",
    "    print('[!] Using original dataset.')\n",
    "    X_train = X_train0#.copy()\n",
    "    y_train = y_train0#.copy()\n",
    "\n",
    "    X_test = X_test0\n",
    "    y_test = y_test0\n",
    "\n",
    "else:\n",
    "    \n",
    "    if RESAMPLED_PRISONERS==True:\n",
    "        print('[!] Using resampled Iowa prisoners dataset.')\n",
    "        X_train = X_train_sm#.copy()\n",
    "        y_train = y_train_sm#.copy()\n",
    "\n",
    "    else:\n",
    "        print('[!] Using imbalanced Iowa prisoners dataset .')\n",
    "        X_train = X_train_df\n",
    "        y_train = y_tr\n",
    "    \n",
    "    ## Rename X,y test\n",
    "    X_test = X_test_df\n",
    "    y_test = y_te\n",
    "\n",
    "# print('\\tX_train and X_test:')\n",
    "# print(X_train.shape, X_test.shape)\n",
    "\n",
    "# print('\\ty_train and y_test')\n",
    "# print(y_train.shape,y_test.shape)\n",
    "\n",
    "print(f\"\\nX_train shape: {X_train.shape}\")\n",
    "\n",
    "print('\\ny_train class balance:')\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First: Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-13T15:17:07.099Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "tic = time() #timing!\n",
    "\n",
    "svc_linear = SVC(kernel='linear', C=1)\n",
    "svc_linear.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = svc_linear.predict(X_train)\n",
    "y_pred_test = svc_linear.predict(X_test)\n",
    "\n",
    "toc = time()\n",
    "print(f\"Run time is {toc-tic} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-13T15:17:07.101Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# how'd we do?\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix , accuracy_score\n",
    "\n",
    "print(classification_report(y_test, y_pred_test)) \n",
    "print(f\"Train accuracy: {accuracy_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_pred_test):.4f}\")\n",
    "\n",
    "plot_confusion_matrix(svc_linear, X_test, y_test,cmap='Blues',normalize='true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now: RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-13T15:17:07.104Z"
    }
   },
   "outputs": [],
   "source": [
    "tic = time() #timing!\n",
    "\n",
    "svc_rbf = SVC(kernel='rbf', C=1, gamma='scale') # using all default values here\n",
    "svc_rbf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = svc_rbf.predict(X_train)\n",
    "y_pred_test = svc_rbf.predict(X_test)\n",
    "\n",
    "toc = time()\n",
    "print(f\"Run time is {toc-tic} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-13T15:17:07.106Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# how'd we do?\n",
    "print(classification_report(y_test, y_pred_test)) \n",
    "print(f\"Train accuracy: {accuracy_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_pred_test):.4f}\")\n",
    "\n",
    "plot_confusion_matrix(svc_rbf, X_test, y_test,cmap='Blues',normalize='true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And a Polynomial Kernel for good measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-13T15:17:07.109Z"
    }
   },
   "outputs": [],
   "source": [
    "tic = time() #timing!\n",
    "\n",
    "svc_poly = SVC(kernel='poly', C=1, gamma='scale', degree=3) # using mostly default values here\n",
    "svc_poly.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = svc_poly.predict(X_train)\n",
    "y_pred_test = svc_poly.predict(X_test)\n",
    "\n",
    "toc = time()\n",
    "print(f\"Run time is {toc-tic} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-13T15:17:07.111Z"
    }
   },
   "outputs": [],
   "source": [
    "# how'd we do?\n",
    "print(classification_report(y_test, y_pred_test)) \n",
    "print(f\"Train accuracy: {accuracy_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_pred_test):.4f}\")\n",
    "\n",
    "plot_confusion_matrix(svc_poly, X_test, y_test,cmap='Blues',normalize='true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting C?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-13T15:17:07.114Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "for c in [.01, 1, 100]: \n",
    "    svc_c = SVC(kernel='linear', C=c, gamma='scale') # going linear again\n",
    "    svc_c.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_train = svc_c.predict(X_train)\n",
    "    y_pred_test = svc_c.predict(X_test)\n",
    "\n",
    "    # how'd we do?\n",
    "    print(\"-----\")\n",
    "    print(f'Results at C = {c}')\n",
    "    print(classification_report(y_test, y_pred_test)) \n",
    "    print(f\"Train accuracy: {accuracy_score(y_train, y_pred_train):.4f}\")\n",
    "    print(f\"Test accuracy: {accuracy_score(y_test, y_pred_test):.4f}\")\n",
    "\n",
    "    plot_confusion_matrix(svc_c, X_test, y_test,cmap='Blues',normalize='true')\n",
    "    plt.show()\n",
    "    \n",
    "end = time()\n",
    "print(f\"Run time is {end-start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros \n",
    "\n",
    "- Good for datasets with more variables than observations\n",
    "- Robust against outliers\n",
    "- Good performance\n",
    "- Good off-the-shelf model in general for several scenarios\n",
    "- Can approximate complex non-linear functions\n",
    "\n",
    "## Cons \n",
    "\n",
    "- Long training time required\n",
    "- Tuning required to determine optimal kernel for non-linear SVMs\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Scaled features\n",
    "- Null values filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [An Idiot's Guide to Support Vector Machines (SVMs) from MIT](https://web.mit.edu/6.034/wwwbob/svm.pdf)\n",
    "- [Machine Learning Mastery's Post on Support Vector Machines for Machine Learning](https://machinelearningmastery.com/support-vector-machines-for-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix/IF there's time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using dictionaries to record the training times of all models for all datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comparing training times across datasets and kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-13T15:17:07.118Z"
    }
   },
   "outputs": [],
   "source": [
    "# results = {}\n",
    "# results['example'] = {'data':{'PRISONERS':PRISONERS,\n",
    "#                              \"REWSAMPLED\":RESAMPLED_PRISONERS},\n",
    "#                       'time': None}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env-new",
   "language": "python",
   "name": "learn-env-new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "193.16px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
